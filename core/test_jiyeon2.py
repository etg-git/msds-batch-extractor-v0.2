import re
from pathlib import Path

import pdfplumber
from difflib import SequenceMatcher
from pdf2image.exceptions import PDFInfoNotInstalledError
# OCR
# pip install pdf2image pytesseract pillow
from pdf2image import convert_from_path
import pytesseract
from PIL import Image

POPPLER_PATH = r"C:\Program Files\poppler\poppler-25.07.0\Library\bin"   # ë˜ëŠ” r"C:\Program Files\poppler\bin"
ENABLE_OCR = True     # OCR ì“¸ì§€ ì—¬ë¶€ (Poppler ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ False ì²˜ë¦¬)

# Windows í•„ìš”ì‹œ ê²½ë¡œ ì„¤ì •
# pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

TESS_LANG = "kor+eng"
OCR_DPI = 300
OCR_TEXT_MIN_CHARS = 40  # í˜ì´ì§€ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì´ ê°’ ë¯¸ë§Œì´ë©´ í•´ë‹¹ í˜ì´ì§€ë§Œ OCR

# â”€â”€ ê³µë°±/êµ¬ë¶„ì ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¨ì–´ ì‚¬ì´ êµ¬ë¶„ì: ì¼ë°˜ ê³µë°± + NBSP/ì œë¡œí­ ê³µë°± + êµ¬ë¶„ì ë“¤
sep = r"[\s\u00A0\u2000-\u200B\.\-Â·ãƒ»,ï¼/]*"
# ë²ˆí˜¸ ì ‘ë‘ë¶€/ì‚¬ì´/ë’¤ì— í—ˆìš©í•  ê³µë°± í´ë˜ìŠ¤
WS = r"[\s\u00A0\u2000-\u200B]*"

SECNUM = {
    "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": 1,
    "ìœ í•´ì„±ìœ„í—˜ì„±": 2,
    "êµ¬ì„±ì„±ë¶„": 3,
    "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": 9,
    "ë²•ì ê·œì œ": 15,
}

# ê° ì„¹ì…˜ì—ì„œ "ê°™ì´" ìˆì–´ì•¼ í•˜ëŠ”(ë˜ëŠ” ìˆìœ¼ë©´ ì¢‹ì€) í‚¤ì›Œë“œ ì„¸íŠ¸
PROB_KEYS = {
    1: (["í™”í•™","ì œí’ˆ","íšŒì‚¬","ì •ë³´","ì œí’ˆëª…"], ["ì œí’ˆ","íšŒì‚¬","ì •ë³´"]),
    2: (["ìœ í•´","ìœ„í—˜"], ["ìœ í•´ì„±","ìœ„í—˜ì„±","ìœ í•´ìœ„í—˜"]),
    3: (["êµ¬ì„±","ì„±ë¶„","í•¨ëŸ‰","í•¨ìœ ","ì¡°ì„±"], ["ì„±ë¶„","í•¨ëŸ‰","í•¨ìœ ","ì¡°ì„±"]),
    9: (["ë¬¼ë¦¬","í™”í•™","íŠ¹ì„±","íŠ¹ì§•"], ["ë¬¼ë¦¬í™”í•™","íŠ¹ì„±","íŠ¹ì§•"]),
    15: (["ë²•ì ","ë²•ê·œ"], ["ê·œì œ","ê·œì¡”","ê·œì œí˜„í™©","ê·œì¡”í˜„í™©"]),
}

def is_probably_section_line(line: str, num: int) -> bool:
    """ì£¼ì–´ì§„ ë¼ì¸ì´ 'ì„¹ì…˜ ë²ˆí˜¸ + í•µì‹¬ í‚¤ì›Œë“œ(AND, í¼ì§€ í—ˆìš©)'ë¥¼ ë§Œì¡±í•˜ë©´ True"""
    s = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
    # 1) ë²ˆí˜¸
    if not re.search(sec(num), s):
        return False
    # 2) í‚¤ì›Œë“œ AND-ish (must ì¤‘ 1ê°œ ì´ìƒ + also ì¤‘ 1ê°œ ì´ìƒ)
    must, also = PROB_KEYS.get(num, ([], []))
    return contains_near(s, must) and contains_near(s, also)
  
def similar(a, b):
    a = re.sub(r"[\s\u00A0\u2000-\u200B]+", "", a or "")
    b = re.sub(r"[\s\u00A0\u2000-\u200B]+", "", b or "")
    return SequenceMatcher(None, a, b).ratio()

def contains_near(line: str, targets: list[str], threshold=0.78) -> bool:
    """ë¼ì¸ì— targets ì¤‘ í•˜ë‚˜ë¼ë„ ìœ ì‚¬í•˜ê²Œ í¬í•¨ë˜ë©´ True"""
    hay = re.sub(r"\s+", "", line)
    for t in targets:
        if t in hay:
            return True
        # í† í°ì„ ìª¼ê°œì„œ ê·¼ì‚¬ íƒìƒ‰
        for w in re.split(r"[^\wê°€-í£]+", hay):
            if w and similar(w, re.sub(r"\s+", "", t)) >= threshold:
                return True
    return False

def is_probably_legal_section_line(line: str) -> bool:
    """15ë²ˆ ì„¹ì…˜ í—¤ë”ë¥¼ ì˜¤íƒ€ê¹Œì§€ ANDë¡œ ê°ì§€ (ë²ˆí˜¸ + ë²•ì /ë²•ê·œ + ê·œì œ ê³„ì—´)"""
    s = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
    # 1) ë²ˆí˜¸(sec15) ë“¤ì–´ìˆê³ 
    if not re.search(sec(15), s):
        return False
    # 2) 'ë²•ì ' ë˜ëŠ” 'ë²•ê·œ'ë¥¼ ìœ ì‚¬ë„ í—ˆìš©ìœ¼ë¡œ í¬í•¨í•˜ê³ 
    if not contains_near(s, ["ë²•ì ", "ë²•ê·œ"]):
        return False
    # 3) 'ê·œì œ'ë¥¼ ìœ ì‚¬ë„ í—ˆìš©ìœ¼ë¡œ í¬í•¨ (ê·œì¡”, ê·œì œí˜„í™© ë“± ì»¤ë²„)
    if not contains_near(s, ["ê·œì œ", "ê·œì œí˜„í™©", "ê·œì¡”", "ê·œì¡”í˜„í™©"]):
        return False
    return True


def _print_box(title: str):
    print("\n" + "="*100)
    print(f"ğŸ” {title}")
    print("="*100)

def _show_context(lines, idx, radius=3):
    s = max(0, idx - radius)
    e = min(len(lines), idx + radius + 1)
    for i in range(s, e):
        mark = ">>" if i == idx else "  "
        print(f"{mark} [{i:04d}] {lines[i][:200]}")

def debug_dump_patterns(section_patterns, fallback_rxs):
    _print_box("ì„¹ì…˜ íŒ¨í„´(ë¼ì¸ ê¸°ë°˜) & Fallback(ë©€í‹°ë¼ì¸)")
    for k, pats in section_patterns.items():
        print(f"\n[{k}] ë¼ì¸ ê¸°ë°˜ íŒ¨í„´ {len(pats)}ê°œ")
        for j, p in enumerate(pats, 1):
            print(f"  ({j}) {p}")
        if k in fallback_rxs:
            print(f"  â†³ Fallback: {fallback_rxs[k].pattern}")

def debug_try_line_match(lines, pats, title="(ë¼ì¸ ê¸°ë°˜ ì •ê·œì‹)"):
    hit_idxs = []
    for i, line in enumerate(lines):
        line_cmp = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
        for p in pats:
            if re.search(p, line_cmp, re.IGNORECASE):
                hit_idxs.append(i)
                break
    print(f"  - {title} ë§¤ì¹˜ ë¼ì¸ ìˆ˜: {len(hit_idxs)}")
    if hit_idxs:
        print("  - ì²« 3ê°œ í›„ë³´:")
        for i in hit_idxs[:3]:
            _show_context(lines, i, radius=1)
    return hit_idxs

def debug_try_number_only(lines, n):
    print(f"  - ë²ˆí˜¸í—¤ë” sec({n})ë§Œ ë§¤ì¹­ë˜ëŠ” ë¼ì¸(ì˜¤íƒ ê°€ëŠ¥) ì²´í¬")
    rx = re.compile(sec(n), re.IGNORECASE)
    hits = [i for i, ln in enumerate(lines) if rx.search(re.sub(r"[\u00A0\u2000-\u200B]", " ", ln))]
    print(f"    Â· ë§¤ì¹˜ {len(hits)}ê°œ")
    for i in hits[:3]:
        _show_context(lines, i, 1)
    return hits

def debug_try_keyword_only(lines, keyword_regex, title="í‚¤ì›Œë“œë§Œ"):
    print(f"  - {title} ë§¤ì¹­ ë¼ì¸(ë²ˆí˜¸ ì—†ì´ í‚¤ì›Œë“œë§Œ ìˆëŠ” ì¤„) ì²´í¬")
    rx = re.compile(keyword_regex, re.IGNORECASE)
    hits = [i for i, ln in enumerate(lines) if rx.search(re.sub(r"[\u00A0\u2000-\u200B]", " ", ln))]
    print(f"    Â· ë§¤ì¹˜ {len(hits)}ê°œ")
    for i in hits[:3]:
        _show_context(lines, i, 1)
    return hits

def debug_try_fallback(full_text, rx, lines, title="Fallback"):
    print(f"  - {title} ë©€í‹°ë¼ì¸ ê²€ìƒ‰")
    txt = re.sub(r"[\u00A0\u2000-\u200B]", " ", full_text)
    m = rx.search(txt)
    if not m:
        print("    Â· ë§¤ì¹˜ ì—†ìŒ")
        return -1
    idx = txt[:m.start()].count("\n")
    print(f"    Â· ë§¤ì¹˜ ì‹œì‘ ì¤„ index = {idx}")
    _show_context(lines, idx, 2)
    return idx

def debug_next_boundary(lines, start_idx, next_num):
    print(f"  - ë‹¤ìŒ ë²ˆí˜¸ ê²½ê³„ íƒìƒ‰: {next_num}")
    end_idx = find_next_boundary_for(lines, start_idx, next_num)
    if end_idx == len(lines):
        print("    Â· ë‹¤ìŒ ë²ˆí˜¸ ê²½ê³„ ë¯¸ë°œê²¬(ë¬¸ì„œ ëê¹Œì§€)")
    else:
        print(f"    Â· ê²½ê³„ ë¼ì¸ index = {end_idx}")
        _show_context(lines, end_idx, 1)
    return end_idx

def debug_toc_pages(pdf_path: str):
    print("\n" + "-"*60)
    print("ğŸ“„ í˜ì´ì§€ë³„ TOC(ëª©ì°¨) íŒì • ìš”ì•½")
    print("-"*60)
    with pdfplumber.open(pdf_path) as pdf:
        for pi, page in enumerate(pdf.pages, 1):
            t = page.extract_text() or ""
            t = strip_page_edges(t)
            flag = is_toc_page(t)
            print(f"  p{pi:02d}  TOC={flag}   (chars={len(t)})")
            if flag:
                # ëª©ì°¨ë¡œ ë³¸ ê²½ìš° ì• ëª‡ ì¤„ë§Œ ë³´ì—¬ì£¼ê¸°
                lines = [ln for ln in t.split("\n") if ln.strip()]
                for ln in lines[:5]:
                    print("     Â·", ln[:200])

def run_debug(pdf_path: str, section_keys=None):
    """
    section_keys: ["ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±","ë²•ì ê·œì œ"] ë“±. Noneì´ë©´ 1/2/3/9/15 ëª¨ë‘
    """
    if section_keys is None:
        section_keys = ["í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´", "ìœ í•´ì„±ìœ„í—˜ì„±", "êµ¬ì„±ì„±ë¶„", "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±", "ë²•ì ê·œì œ"]

    # 0) í˜ì´ì§€ë³„ TOC íŒì • ì°¸ê³  (ëª©ì°¨ë¡œ ì œê±°ë˜ëŠ”ì§€ ì‹œê°í™”)
    debug_toc_pages(pdf_path)

    # 1) ì›ë¬¸ í…ìŠ¤íŠ¸/í´ë¦° í…ìŠ¤íŠ¸ í™•ë³´
    page_texts = extract_text_pages_hybrid(pdf_path)
    full_raw = "\n".join(page_texts)             # strip_toc_block ì ìš© ì „
    lines_raw = full_raw.split("\n")

    lines = remove_repeated_headers(lines_raw)
    lines = strip_toc_block(lines)
    full_clean = "\n".join(lines)

    # 2) íŒ¨í„´ ë¤í”„
    section_patterns = find_section_patterns()
    debug_dump_patterns(section_patterns, FALLBACK_HEAD_RXS)

    # 3) ì„¹ì…˜ë³„ ë””ë²„ê·¸
    for key in section_keys:
        _print_box(f"ì„¹ì…˜ ë””ë²„ê¹…: {key}")
        pats = section_patterns[key]

        print(" (A) ë¼ì¸ ê¸°ë°˜: í´ë¦°í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œì‹ íƒìƒ‰")
        hit_idxs = debug_try_line_match(lines, pats)

        # ë²ˆí˜¸ë§Œ / í‚¤ì›Œë“œë§Œ ì²´í¬(ì˜¤íƒ/ëˆ„ë½ ìœ í˜• íŒŒì•…)
        if key == "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±":
            debug_try_number_only(lines, 9)
            debug_try_keyword_only(lines, r"(ë¬¼ë¦¬\s*í™”í•™\s*ì |ë¬¼ë¦¬\s*í™”í•™|ë¬¼ë¦¬\s*ì )\s*(íŠ¹ì„±|íŠ¹ì§•)", "ë¬¼ë¦¬/í™”í•™ í‚¤ì›Œë“œ")
        elif key == "ë²•ì ê·œì œ":
            debug_try_number_only(lines, 15)
            debug_try_keyword_only(lines, r"(ë²•ì |ë²•ê·œ)\s*ê·œì œ(\s*í˜„í™©)?", "ë²•ì /ê·œì œ í‚¤ì›Œë“œ")

        # (B) Fallback: ì›ë¬¸ -> í´ë¦° ìˆœì„œë¡œ ì‹œë„
        print(" (B) ë©€í‹°ë¼ì¸ Fallback: ì›ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰")
        fb_idx_raw = debug_try_fallback(full_raw, FALLBACK_HEAD_RXS[key], lines_raw, "Fallback(raw)")

        print(" (C) ë©€í‹°ë¼ì¸ Fallback: í´ë¦° í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰")
        fb_idx_clean = debug_try_fallback(full_clean, FALLBACK_HEAD_RXS[key], lines, "Fallback(clean)")

        # (D) ê²½ê³„ í™•ì¸(ì‹œì‘ í›„ë³´ê°€ ìˆì„ ë•Œë§Œ)
        start_idx = None
        if hit_idxs:
            start_idx = hit_idxs[0]
        elif fb_idx_clean != -1:
            start_idx = fb_idx_clean
        elif fb_idx_raw != -1:
            # raw ê¸°ì¤€ ì¤„ ë²ˆí˜¸ë¥¼ clean ê¸°ì¤€ìœ¼ë¡œ ê·¼ì‚¬ ë§¤í•‘(ì™„ë²½í•˜ì§„ ì•Šì§€ë§Œ ë§¥ë½ í™•ì¸ìš©)
            start_idx = min(len(lines)-1, fb_idx_raw)

        if start_idx is not None:
            if key in BOUNDARY_NEXT_NUMBER:
                debug_next_boundary(lines, start_idx, BOUNDARY_NEXT_NUMBER[key])
            else:
                print("  - ê²½ê³„ íƒìƒ‰ ì—†ìŒ(íƒ€ê¹ƒ ì„¹ì…˜ ì•„ë‹˜)")
        else:
            print("  - ì‹œì‘ í›„ë³´ ìì²´ê°€ ì—†ì–´ ê²½ê³„ íƒìƒ‰ ìƒëµ")

def sec(n: int) -> str:
    """
    í–‰ ì‹œì‘ ë²ˆí˜¸ í‘œê¸° í—ˆìš©:
    - [9], 9., 9), 9-, 9:, 9 (êµ¬ë¶„ì ì—†ì´ ê³µë°±ë§Œ)
    - ì „ê° ì /ì½œë¡ /ì¼ë³¸ì–´ ë§ˆì¹¨í‘œ í—ˆìš©: ï¼ ï¼š ã€‚
    - ì œ 9 ì¥/í•­
    - ì¶”ê°€: ìˆ«ì ì•ì˜ ë°±í‹±/ë”°ì˜´í‘œ/ë¶ˆë¦¿ ë“± ì¡ë¬¸ì í—ˆìš©
    """
    punc = r"[\.\)\-:ï¼šï¼ã€‚]"
    lead = r"[\s\u00A0\u2000-\u200B`\uFEFF\"'â€œâ€â€˜â€™Â·â€¢â€“â€”-]*"   # â† ì¶”ê°€
    return rf"^{lead}(?:\[?{n}\]?|{n}{WS}(?:{punc})?{WS}|ì œ?{WS}{n}{WS}[ì¥í•­]){WS}"

def normalize_text(text: str) -> str:
    return re.sub(r"\s+", "", (text or "").lower())

# â”€â”€ ì „ì—­ ë°˜ë³µ í—¤ë”/í‘¸í„°(ë¬¸ì„œ ì „ë°˜ì—ì„œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_header_line(line: str) -> bool:
    normalized = normalize_text(line)
    header_patterns = [
        r"msdsë²ˆí˜¸", r"ë¬¸ì„œë²ˆí˜¸", r"ê°œì •ì¼ì", r"ê°œì •ë²ˆí˜¸",
        r"ë¬¼ì§ˆì•ˆì „ë³´ê±´ìë£Œ", r"materialsafetydatasheets",
        r"ghs[\-\s]?msds",
        r"\d+\s*/\s*\d+\s*(í˜ì´ì§€|page)",
        r"page\s*\d+\s*/\s*\d+",
        r"-\d+/\d+-\s*rev\.", r"rev\.\s*\d+",
        r"copyright", r"all\s*rights\s*reserved",
    ]
    return any(re.search(p, normalized) for p in header_patterns)

def remove_repeated_headers(lines):
    """ë¬¸ì„œ ì•ë¶€ë¶„ì—ì„œ ê°ì§€ëœ ë°˜ë³µ ë¼ì¸ì„ ì „ì²´ì—ì„œ ì œê±°"""
    if not lines:
        return lines
    header_lines = set()
    for line in lines[:10]:
        if is_header_line(line):
            header_lines.add(normalize_text(line))
    return [ln for ln in lines if normalize_text(ln) not in header_lines]

# â”€â”€ í˜ì´ì§€ ê°€ì¥ìë¦¬(ìƒÂ·í•˜ë‹¨) ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PAGE_MARK_RE = re.compile(r"\b\d+\s*/\s*\d+\s*(?:í˜ì´ì§€|page)\b", re.IGNORECASE)
DOC_MARK_RE  = re.compile(r"ghs[\-\s]?msds", re.IGNORECASE)

def strip_page_edges(text: str) -> str:
    """ê° í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ì²« 3ì¤„/ë§ˆì§€ë§‰ 3ì¤„ì˜ í—¤ë”Â·í‘¸í„° ì œê±°"""
    lines = text.split("\n") if text else []
    if not lines:
        return text
    new = []
    for i, ln in enumerate(lines):
        at_top = i < 3
        at_bot = i >= len(lines) - 3
        if (at_top and (DOC_MARK_RE.search(ln) or is_header_line(ln))) \
           or (at_bot and (PAGE_MARK_RE.search(ln) or is_header_line(ln))):
            continue
        new.append(ln)
    return "\n".join(new)

# â”€â”€ ëª©ì°¨(TOC) ê°ì§€/ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOC_HINT_WORDS = {
    "ëª©ì°¨", "contents", "table of contents", "ghs-msds", "ë¬¼ì§ˆ ì•ˆì „ë³´ê±´ìë£Œ",
}
TOC_SECTION_KEYS = {
    "í™”í•™", "íšŒì‚¬", "ìœ í•´", "ìœ„í—˜", "êµ¬ì„±", "ì‘ê¸‰", "í­ë°œ", "ëˆ„ì¶œ",
    "ì·¨ê¸‰", "ë³´ê´€", "ë…¸ì¶œ", "ë³´í˜¸êµ¬", "ë¬¼ë¦¬", "í™”í•™ì ", "ì•ˆì •ì„±", "ë°˜ì‘ì„±",
    "ë…ì„±", "í™˜ê²½", "íê¸°", "ìš´ì†¡", "ë²•ì ", "ê·œì œ", "ê¸°íƒ€", "ì°¸ê³ "
}

def is_toc_like_numbering(line: str) -> int:
    """ëª©ì°¨ ìˆ«ì ë¼ì¸: '1. â€¦', '10) â€¦', '[15] â€¦' í˜•íƒœë©´ ë²ˆí˜¸ ë°˜í™˜, ì•„ë‹ˆë©´ -1"""
    m = re.match(r"^\s*(?:\[(\d{1,2})\]|(\d{1,2})\s*[\.\):])", line)
    if not m:
        return -1
    n = m.group(1) or m.group(2)
    try:
        val = int(n)
        return val if 1 <= val <= 16 else -1
    except:
        return -1

def is_toc_page(text: str) -> bool:
    """í˜ì´ì§€ ì „ì²´ê°€ ëª©ì°¨/í‘œì§€ë¡œ ë³´ì´ë©´ True (ì¡°ê¸ˆ ë” ê°•í•˜ê²Œ)"""
    if not text:
        return False
    t = text.strip()
    lines = [ln for ln in t.split("\n") if ln.strip()]

    # íŒíŠ¸ ë‹¨ì–´
    hint = any(h.lower() in t.lower() for h in TOC_HINT_WORDS)

    # ë²ˆí˜¸ í˜•íƒœ(1., 10), [15])ê°€ ì—¬ëŸ¬ ê°œ ë‚˜ì˜¤ë©´ ëª©ì°¨ì— ê°€ê¹Œì›€
    nums = []
    numbered_lines = 0
    for ln in lines:
        n = is_toc_like_numbering(ln)
        if n != -1:
            nums.append(n)
            numbered_lines += 1
    unique_nums = set(nums)

    # ì„¹ì…˜ í‚¤ì›Œë“œ ë‹¤ìˆ˜
    kw_hits = sum(any(kw in ln for kw in TOC_SECTION_KEYS) for ln in lines)
    kw_ratio = kw_hits / max(1, len(lines))

    # ê°•í™”ëœ ê¸°ì¤€:
    #  - íŒíŠ¸ ë‹¨ì–´ê°€ ìˆê±°ë‚˜
    #  - ì„œë¡œ ë‹¤ë¥¸ ë²ˆí˜¸ê°€ 6ê°œ ì´ìƒ(ìµœëŒ€ â‰¤16)ì´ê³ , ë²ˆí˜¸ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ë¹„ìœ¨ì´ 30% ì´ìƒì´ë©°, ì„¹ì…˜ í‚¤ì›Œë“œë„ ì–´ëŠ ì •ë„ ìˆìŒ
    return (
        hint
        or (
            len(unique_nums) >= 6
            and (max(unique_nums, default=0) <= 16)
            and (numbered_lines / max(1, len(lines)) >= 0.30)
            and kw_ratio >= 0.10
        )
    )

# â”€â”€ ì„¹ì…˜ íŒ¨í„´(ëŠìŠ¨í•˜ê²Œ ë³´ê°•) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_section_patterns():
    return {
        "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": [
            sec(1) + rf"í™”í•™{sep}ì œí’ˆ{sep}ê³¼{sep}íšŒì‚¬(?:{sep}ì—{sep}ê´€í•œ{sep}ì •ë³´)?",
            sec(1) + rf"í™”í•™{sep}ì œí’ˆ",
            sec(1) + rf"ì œí’ˆ{sep}ëª…",
            sec(1) + rf"í™”í•™{sep}íšŒì‚¬",
        ],
        "ìœ í•´ì„±ìœ„í—˜ì„±": [
            # ìœ í•´ â†’ ìœ„í—˜
            sec(2) + rf"ìœ í•´{sep}ì„±{sep}[Â·ãƒ»\.]?{sep}ìœ„í—˜{sep}ì„±",
            sec(2) + rf"ìœ í•´{sep}ìœ„í—˜{sep}ì„±",
            sec(2) + rf"ìœ í•´{sep}ì„±", sec(2) + rf"ìœ í•´{sep}ìœ„í—˜",
            # ìœ„í—˜ â†’ ìœ í•´ (ì—­ìˆœ)
            sec(2) + rf"ìœ„í—˜{sep}ì„±{sep}[Â·ãƒ»\.]?{sep}ìœ í•´{sep}ì„±",
            sec(2) + rf"ìœ„í—˜{sep}ìœ í•´{sep}ì„±", sec(2) + rf"ìœ„í—˜{sep}ìœ í•´",
            # 'ë°' ì—°ê²°í˜•
            sec(2) + rf"(?:ìœ í•´|ìœ„í—˜){sep}ì„±{sep}ë°{sep}(?:ìœ í•´|ìœ„í—˜){sep}ì„±",
            sec(2) + rf"(?:ìœ í•´|ìœ„í—˜){sep}ë°{sep}(?:ìœ í•´|ìœ„í—˜){sep}ì„±",
        ],
        "êµ¬ì„±ì„±ë¶„": [
            # ë‹¤ì–‘í•œ ê¼¬ë¦¬ë§/ë™ì˜ì–´: í•¨ìœ ëŸ‰/í•¨ëŸ‰/ì¡°ì„±/ì„±ë¶„í‘œ
            sec(3) + rf"êµ¬ì„±{sep}ì„±ë¶„(?:{sep}ì˜{sep}ëª…ì¹­{sep}ë°{sep}(?:í•¨ìœ ?{sep}?ëŸ‰|í•¨ëŸ‰|ì¡°ì„±))?",
            sec(3) + rf"(?:êµ¬ì„±{sep})?ì„±ë¶„{sep}(?:í‘œ|ì •ë³´)?",
            sec(3) + rf"ì„±ë¶„{sep}(?:ëª…|ëª…ì¹­){sep}ë°{sep}(?:í•¨ìœ ?{sep}?ëŸ‰|í•¨ëŸ‰)",
            sec(3) + rf"ì¡°ì„±{sep}(?:ë°{sep}ëª…ì¹­|ì •ë³´|í‘œ)?",
        ],
        "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": [
            # íŠ¹ì„±/íŠ¹ì§• ëª¨ë‘ í—ˆìš©
            sec(9) + rf"ë¬¼ë¦¬{sep}í™”í•™{sep}?ì {sep}(?:íŠ¹ì„±|íŠ¹ì§•)",
            sec(9) + rf"ë¬¼ë¦¬{sep}í™”í•™{sep}(?:íŠ¹ì„±|íŠ¹ì§•)",
            sec(9) + rf"ë¬¼ë¦¬{sep}ì {sep}(?:íŠ¹ì„±|íŠ¹ì§•)",
        ],
        "ë²•ì ê·œì œ": [
            # 'ê·œì œ' â†” 'ê·œì¡”' ì˜¤íƒ€ í—ˆìš©
            sec(15) + rf"(?:ë²•ì |ë²•\s*ê·œ){sep}ê·œ[ì œì¡”](?:{sep}í˜„í™©)?",
            # ì—¬ìœ  íŒ¨í„´: 'ê´€ë ¨/ê¸°íƒ€ ë²•/ê·œ ì œ' ë“±
            sec(15) + rf"(?:ê´€ë ¨|ê¸°\s*íƒ€)?{sep}(?:ë²•|ê·œ){sep}ì œ",
            # ë…¸ê³¨ì ìœ¼ë¡œ 'ê·œì¡”'ë§Œ ì°íŒ ê²½ìš°ë„ ì»¤ë²„
            sec(15) + rf"(?:ë²•ì |ë²•\s*ê·œ){sep}ê·œì¡”(?:{sep}í˜„í™©)?",
        ],
    }

# â”€â”€ ìœ ì‚¬ë„(ë°±ì—…) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FUZZY_CANDIDATES = {
    "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": ["í™”í•™ ì œí’ˆê³¼ íšŒì‚¬", "í™”í•™ì œí’ˆ", "ì œí’ˆ ëª…", "í™”í•™ íšŒì‚¬", "íšŒì‚¬ ì •ë³´"],
    "ìœ í•´ì„±ìœ„í—˜ì„±": ["ìœ í•´ ìœ„í—˜ì„±", "ìœ„í—˜ ìœ í•´ì„±", "ìœ í•´ì„±", "ìœ„í—˜ì„±", "ìœ í•´ ìœ„í—˜"],
    "êµ¬ì„±ì„±ë¶„": ["êµ¬ì„± ì„±ë¶„", "ì„±ë¶„í‘œ", "ì„±ë¶„ í•¨ìœ ëŸ‰", "ì„±ë¶„ í•¨ëŸ‰", "ì¡°ì„± ì„±ë¶„"],
    "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": ["ë¬¼ë¦¬ í™”í•™ì  íŠ¹ì„±", "ë¬¼ë¦¬ í™”í•™ì  íŠ¹ì§•", "ë¬¼ë¦¬. í™”í•™ì  íŠ¹ì„±", "ë¬¼ë¦¬Â·í™”í•™ì  íŠ¹ì„±"],
    "ë²•ì ê·œì œ": ["ë²•ì  ê·œì œ", "ë²•ì  ê·œì œ í˜„í™©", "ë²•ê·œ ê·œì œ", "ë²•ê·œ ê·œì œ í˜„í™©"],
}


def find_all_section_starts(lines, patterns):
    """í•´ë‹¹ ì„¹ì…˜ì˜ ëª¨ë“  ì‹œì‘ í›„ë³´ ë¼ì¸ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜"""
    idxs = []
    for i, line in enumerate(lines):
        line_cmp = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
        for pattern in patterns:
            if re.search(pattern, line_cmp, re.IGNORECASE):
                idxs.append(i)
                break
    return idxs

def count_body_lines_between(lines, start_idx, end_idx):
    """í—¤ë”/ë¹ˆì¤„ ì œì™¸ ì‹¤ë‚´ìš© ë¼ì¸ìˆ˜ë¥¼ ìƒŒë‹¤"""
    cnt = 0
    for line in lines[start_idx+1:end_idx]:
        if line.strip() and not is_header_line(line):
            cnt += 1
    return cnt

def select_best_start(lines, candidate_idxs, section_name):
    """
    í›„ë³´ë“¤ ì¤‘ ë³¸ë¬¸ ë¼ì¸ìˆ˜ê°€ ê°€ì¥ ë§ì€ ê²ƒì„ ì„ íƒ.
    - BOUNDARY_NEXT_NUMBERê°€ ìˆìœ¼ë©´ ê·¸ ë²ˆí˜¸ë¡œ ì •í™• ê²½ê³„ ê³„ì‚°
    - ì—†ìœ¼ë©´ ë‹¤ìŒ í›„ë³´/ë¬¸ì„œ ëê¹Œì§€
    - ë™ì ì´ë©´ ê°€ì¥ ë’¤ìª½(=ë³¸ë¬¸ ê°€ëŠ¥ì„±â†‘)
    - ëª¨ë“  í›„ë³´ ë³¸ë¬¸ì´ ì§§ìœ¼ë©´(<=1ì¤„) ê°€ì¥ ë§ˆì§€ë§‰ í›„ë³´ë¡œ ê°•ì œ ì„ íƒ
    """
    if not candidate_idxs:
        return -1

    best_idx = candidate_idxs[-1]
    best_body = -1

    for s in candidate_idxs:
        # ì¢…ë£Œ ê²½ê³„ ê²°ì •
        if section_name in BOUNDARY_NEXT_NUMBER:
            forced_end = find_next_boundary_for(lines, s, BOUNDARY_NEXT_NUMBER[section_name])
        else:
            # ë‹¤ìŒ í›„ë³´ ì§ì „ê¹Œì§€
            later = [c for c in candidate_idxs if c > s]
            forced_end = (min(later) if later else len(lines))

        body_cnt = count_body_lines_between(lines, s, forced_end)

        # ë³¸ë¬¸ ë¼ì¸ì´ ë” ë§ê±°ë‚˜, ê°™ìœ¼ë©´ ë” ë’¤ìª½ ê²ƒì„ ì„ í˜¸
        if (body_cnt > best_body) or (body_cnt == best_body and s > best_idx):
            best_body = body_cnt
            best_idx = s

    # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(<=1) ë§ˆì§€ë§‰ í›„ë³´(=ë³¸ë¬¸ì¼ ê°€ëŠ¥ì„±â†‘)ë¡œ êµì²´
    if best_body <= 1:
        best_idx = candidate_idxs[-1]

    return best_idx
  
  
def fuzzy_find_section_line(lines, candidates, threshold=0.78):
    best_idx, best_score = -1, 0.0
    for i, line in enumerate(lines):
        # íŠ¹ìˆ˜ê³µë°± í‰íƒ„í™”
        line_clean = re.sub(r"[\s\u00A0\u2000-\u200B]+", "", line)
        for cand in candidates:
            cand_clean = re.sub(r"[\s\u00A0\u2000-\u200B]+", "", cand)
            score = SequenceMatcher(None, line_clean, cand_clean).ratio()
            if score > best_score:
                best_idx, best_score = i, score
    return best_idx if best_score >= threshold else -1

def find_section_start(lines, patterns, section_key=None):
    # 1) ì •ê·œì‹ í›„ë³´ ëª¨ë‘ ìˆ˜ì§‘
    candidates = find_all_section_starts(lines, patterns)

    # 2) ì„¹ì…˜ë³„ AND í¼ì§€ ë³´ì •: ëª¨ë“  ì„¹ì…˜(1/2/3/9/15)ì— ì ìš©
    if not candidates and section_key and section_key in SECNUM:
        secnum = SECNUM[section_key]
        for i, ln in enumerate(lines):
            if is_probably_section_line(ln, secnum):
                candidates.append(i)

    # 3) ê¸°ì¡´ fuzzy ë°±ì—…(ì „ì²´ ë¼ì¸ ìœ ì‚¬ë„)
    if not candidates and section_key and section_key in FUZZY_CANDIDATES:
        idx = fuzzy_find_section_line(
            [re.sub(r"[\s\u00A0\u2000-\u200B]+", "", ln) for ln in lines],
            FUZZY_CANDIDATES[section_key]
        )
        return idx

    # 4) ê°€ì¥ â€˜ë³¸ë¬¸ì´ ë§ì€â€™ í›„ë³´ ì„ íƒ
    return select_best_start(lines, candidates, section_key if section_key else "")

# â”€â”€ ì •í™• ê²½ê³„: 3â†’4, 9â†’10, 15â†’16 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BOUNDARY_NEXT_NUMBER = {"êµ¬ì„±ì„±ë¶„": 4, "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": 10, "ë²•ì ê·œì œ": 16}

def head_only(n: int) -> re.Pattern:
    return re.compile(sec(n) + r".*$", re.IGNORECASE)

def find_next_boundary_for(lines, start_idx, next_num):
    pat = head_only(next_num)
    for i in range(start_idx + 1, len(lines)):
        # íŠ¹ìˆ˜ê³µë°± í‰íƒ„í™” í›„ ê²€ì‚¬
        if pat.search(re.sub(r"[\u00A0\u2000-\u200B]", " ", lines[i])):
            return i
    return len(lines)

# â”€â”€ í˜ì´ì§€ì— ì„¹ì…˜ í—¤ë”ê°€ ìˆìœ¼ë©´ ì ˆëŒ€ ë²„ë¦¬ì§€ ì•Šê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def page_contains_section_head(text: str) -> bool:
    if not text:
        return False
    hay = re.sub(r"[\u00A0\u2000-\u200B]", " ", text)

    # ê¸°ì¡´ ì—„ë°€ íŒ¨í„´
    patterns = find_section_patterns()
    for pats in patterns.values():
        for p in pats:
            if re.search(p, hay, re.IGNORECASE | re.MULTILINE):
                return True

    # ëŠìŠ¨í•œ íŒíŠ¸(ê¸°ì¡´) + 15ë²ˆ ì˜¤íƒ€ ë³´ê°•
    # ... ê¸°ì¡´ loose_hints ê²€ì‚¬ ìœ ì§€ ...
    # ì¶”ê°€: 15ë²ˆ ë¼ì¸ ì˜¤íƒ€ AND ë§¤ì¹­
    for line in hay.splitlines():
        if is_probably_legal_section_line(line):
            return True

    return False

# â”€â”€ ë©€í‹°ë¼ì¸ Fallback (ë¼ì¸ ê²½ê³„/ì œê±° ì´ìŠˆ ëŒ€ë¹„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fallback_find_head(full_text: str, rx: re.Pattern) -> int:
    """ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë©€í‹°ë¼ì¸ ì •ê·œì‹ìœ¼ë¡œ ì œëª© ê²€ìƒ‰ â†’ ì¤„ ì¸ë±ìŠ¤ í™˜ì‚°"""
    txt = re.sub(r"[\u00A0\u2000-\u200B]", " ", full_text)
    m = rx.search(txt)
    if not m:
        return -1
    return txt[:m.start()].count("\n")

FALLBACK_HEAD_RXS = {
    "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": re.compile(
        rf"{sec(1)}(?:í™”í•™{sep}ì œí’ˆ{sep}ê³¼{sep}íšŒì‚¬(?:{sep}ì—{sep}ê´€í•œ{sep}ì •ë³´)?|í™”í•™{sep}ì œí’ˆ|ì œí’ˆ{sep}ëª…|í™”í•™{sep}íšŒì‚¬)",
        re.IGNORECASE | re.MULTILINE
    ),
    "ìœ í•´ì„±ìœ„í—˜ì„±": re.compile(
        rf"{sec(2)}(?:(?:ìœ í•´{sep}ì„±{sep}[Â·ãƒ»\.]?{sep}ìœ„í—˜{sep}ì„±)|(?:ìœ í•´{sep}ìœ„í—˜{sep}ì„±)|(?:ìœ í•´{sep}ì„±)|(?:ìœ í•´{sep}ìœ„í—˜)|"
        rf"(?:ìœ„í—˜{sep}ì„±{sep}[Â·ãƒ»\.]?{sep}ìœ í•´{sep}ì„±)|(?:ìœ„í—˜{sep}ìœ í•´{sep}ì„±)|(?:ìœ„í—˜{sep}ìœ í•´)|"
        rf"(?:(?:ìœ í•´|ìœ„í—˜){sep}ì„±{sep}ë°{sep}(?:ìœ í•´|ìœ„í—˜){sep}ì„±)|(?:(?:ìœ í•´|ìœ„í—˜){sep}ë°{sep}(?:ìœ í•´|ìœ„í—˜){sep}ì„±))",
        re.IGNORECASE | re.MULTILINE
    ),
    "êµ¬ì„±ì„±ë¶„": re.compile(
        rf"{sec(3)}(?:êµ¬ì„±{sep}ì„±ë¶„(?:{sep}ì˜{sep}ëª…ì¹­{sep}ë°{sep}(?:í•¨ìœ ?{sep}?ëŸ‰|í•¨ëŸ‰|ì¡°ì„±))?"
        rf"|(?:êµ¬ì„±{sep})?ì„±ë¶„{sep}(?:í‘œ|ì •ë³´)?|ì„±ë¶„{sep}(?:ëª…|ëª…ì¹­){sep}ë°{sep}(?:í•¨ìœ ?{sep}?ëŸ‰|í•¨ëŸ‰)|ì¡°ì„±.*)",
        re.IGNORECASE | re.MULTILINE
    ),
    "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": re.compile(
        rf"{sec(9)}(?:ë¬¼ë¦¬{sep}í™”í•™{sep}?ì {sep}(?:íŠ¹ì„±|íŠ¹ì§•)|ë¬¼ë¦¬{sep}í™”í•™{sep}(?:íŠ¹ì„±|íŠ¹ì§•)|ë¬¼ë¦¬{sep}ì {sep}(?:íŠ¹ì„±|íŠ¹ì§•))",
        re.IGNORECASE | re.MULTILINE
    ),
    "ë²•ì ê·œì œ": re.compile(
        rf"{sec(15)}(?:ë²•ì |ë²•ê·œ){sep}ê·œ[ì œì¡”](?:{sep}í˜„í™©)?",
        re.IGNORECASE | re.MULTILINE
    ),
}

# â”€â”€ OCR & í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ocr_page_image(image: Image.Image) -> str:
    config = "--psm 3"
    text = pytesseract.image_to_string(image, lang=TESS_LANG, config=config)
    return text or ""

def extract_text_pages_hybrid(pdf_path: str) -> list[str]:
    texts = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            t = page.extract_text() or ""
            t = strip_page_edges(t)
            texts.append(t)

    need_ocr_idx = [i for i, t in enumerate(texts) if len((t or "").strip()) < OCR_TEXT_MIN_CHARS]
    if ENABLE_OCR and need_ocr_idx:
        try:
            # POPPLER_PATHê°€ ì§€ì •ë˜ì—ˆìœ¼ë©´ ì „ë‹¬, ì•„ë‹ˆë©´ ì‹œìŠ¤í…œ PATH ì‚¬ìš©
            kwargs = {"dpi": OCR_DPI}
            if POPPLER_PATH:
                kwargs["poppler_path"] = POPPLER_PATH

            images = convert_from_path(pdf_path, **kwargs)
            for i in need_ocr_idx:
                try:
                    ocr_t = ocr_page_image(images[i])
                    texts[i] = strip_page_edges(ocr_t)
                except Exception as e:
                    print(f"âš ï¸  OCR ì‹¤íŒ¨ (p{i+1}): {e}")
        except PDFInfoNotInstalledError:
            # Poppler ë¯¸ì„¤ì¹˜: í¬ë˜ì‹œ ëŒ€ì‹  ì•ˆë‚´ë§Œ í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œë§Œ ê³„ì†
            print("â“˜ Poppler ë¯¸ì„¤ì¹˜ë¡œ OCRì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤. (í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ)")
        except FileNotFoundError as e:
            # poppler_pathê°€ ì˜ëª»ëœ ê²½ìš° ë“±
            print(f"â“˜ Poppler ì‹¤í–‰íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\n   â†’ POPPLER_PATHë¥¼ ì˜¬ë°”ë¥¸ bin í´ë”ë¡œ ì§€ì •í•˜ì„¸ìš”.")
        except Exception as e:
            print(f"â“˜ OCR ì´ˆê¸°í™” ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ë¡œ OCRì„ ê±´ë„ˆëœë‹ˆë‹¤: {e}")
    # âœ… ì„¹ì…˜ í—¤ë”ê°€ ë³´ì´ë©´ ë¬´ì¡°ê±´ ë³´ì¡´, ê·¸ ë‹¤ìŒì—ë§Œ TOC ì œê±°
    filtered = []
    for t in texts:
        if page_contains_section_head(t):    # â† ì´ ì¡°ê±´ì„ ë°˜ë“œì‹œ ìš°ì„ 
            filtered.append(t)
            continue
        if is_toc_page(t):
            continue                         # â† í—¤ë”ê°€ ì—†ê³  TOCì´ë©´ ë²„ë¦¼
        filtered.append(t)
    return filtered

# â”€â”€ ë” ì•ˆì „í•œ ëª©ì°¨ ë¸”ë¡ ì œê±°(ì„¹ì…˜ í—¤ë” í¬í•¨ ì‹œ ë¯¸ì œê±°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def would_match_any_section_head(line: str) -> bool:
    """í•´ë‹¹ ì¤„ì´ 1/2/3/9/15 ì„¹ì…˜ í—¤ë” íŒ¨í„´ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ True"""
    patterns = find_section_patterns()
    line_cmp = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
    for pats in patterns.values():
        for p in pats:
            if re.search(p, line_cmp, re.IGNORECASE):
                return True
    return False

def strip_toc_block(lines: list[str]) -> list[str]:
    """
    ë³¸ë¬¸ ì† ëª©ì°¨ ë¸”ë¡(ì—°ì† ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸)ì„ ë³´ìˆ˜ì ìœ¼ë¡œ ì œê±°í•˜ë˜,
    ë²„í¼ ì•ˆì— 'ì„¹ì…˜ í—¤ë”(1/2/3/9/15)'ê°€ í•œ ì¤„ì´ë¼ë„ ìˆìœ¼ë©´ ì ˆëŒ€ ì œê±°í•˜ì§€ ì•ŠìŒ.
    """
    out, i, N = [], 0, len(lines)
    while i < N:
        m = re.match(r"^\s*(?:\[(\d{1,2})\]|(\d{1,2})\s*[\.\):])", lines[i])
        if not m:
            out.append(lines[i]); i += 1;  continue

        # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ë²„í¼ë§
        j, uniq, buf = i, set(), []
        while j < N:
            mm = re.match(r"^\s*(?:\[(\d{1,2})\]|(\d{1,2})\s*[\.\):])", lines[j])
            if not mm:
                break
            num = int((mm.group(1) or mm.group(2)))
            uniq.add(num); buf.append(lines[j]); j += 1

        # â¶ ì„¹ì…˜ í—¤ë”ê°€ ë²„í¼ ì•ˆì— ìˆìœ¼ë©´ ì œê±° ê¸ˆì§€
        if any(would_match_any_section_head(b) for b in buf):
            out.extend(buf); i = j;  continue

        # â· ë³´ìˆ˜ì  ê¸°ì¤€
        seq_count = len(buf)
        avg_len = (sum(len(b) for b in buf) / seq_count) if seq_count else 0
        kw_hits = sum(any(kw in b for kw in TOC_SECTION_KEYS) for b in buf)
        is_toc = (seq_count >= 5 and len(uniq) >= 5 and max(uniq) <= 16
                  and avg_len <= 40 and (kw_hits / seq_count) >= 0.5)

        if is_toc:
            i = j  # í†µì§¸ë¡œ ìŠ¤í‚µ
        else:
            out.extend(buf); i = j
    return out

# â”€â”€ ì„¹ì…˜ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_sections(pdf_path: str) -> dict:
    page_texts = extract_text_pages_hybrid(pdf_path)

    # ì œê±° ì „/í›„ í…ìŠ¤íŠ¸ ë‘˜ ë‹¤ ë³´ê´€
    full_text_raw = "\n".join(page_texts)  # strip_toc_block/headers ì ìš© ì „
    lines = full_text_raw.split("\n")

    # ì „ì—­ ë°˜ë³µ í—¤ë”/í‘¸í„° + ë³¸ë¬¸ ì† ëª©ì°¨ ë¸”ë¡ ì œê±°
    lines = remove_repeated_headers(lines)
    lines = strip_toc_block(lines)
    full_text_clean = "\n".join(lines)

    # ì„¹ì…˜ ì‹œì‘ íƒì§€ (ë¼ì¸ ê¸°ë°˜)
    section_patterns = find_section_patterns()
    section_positions = {}
    for section_name, pats in section_patterns.items():
        pos = find_section_start(lines, pats, section_key=section_name)
        if pos != -1:
            section_positions[section_name] = pos

    # â”€â”€ Fallback: 1/2/3/9/15ë¥¼ 'ì œê±° ì „ í…ìŠ¤íŠ¸'ì—ì„œ ë¨¼ì € ë©€í‹°ë¼ì¸ ê²€ìƒ‰
    for key, rx in FALLBACK_HEAD_RXS.items():
        if key not in section_positions:
            idx = fallback_find_head(full_text_raw, rx)
            if idx != -1:
                section_positions[key] = idx

    # ê·¸ë˜ë„ ì—†ìœ¼ë©´ 'ì œê±° í›„ í…ìŠ¤íŠ¸'ì—ì„œë„ ì‹œë„
    for key, rx in FALLBACK_HEAD_RXS.items():
        if key not in section_positions:
            idx = fallback_find_head(full_text_clean, rx)
            if idx != -1:
                section_positions[key] = idx

    if not section_positions:
        return {}

    # ì„¹ì…˜ë³„ ë³¸ë¬¸ ì¶”ì¶œ
    sections = {}
    for section_name, start_pos in sorted(section_positions.items(), key=lambda x: x[1]):
        # ì¢…ë£Œ ìœ„ì¹˜ ê²°ì •
        candidates_after = [p for p in section_positions.values() if p > start_pos]
        default_end = min(candidates_after) if candidates_after else len(lines)
        if section_name in BOUNDARY_NEXT_NUMBER:
            forced_end = find_next_boundary_for(lines, start_pos, BOUNDARY_NEXT_NUMBER[section_name])
            end_pos = min(default_end, forced_end)
        else:
            end_pos = default_end

        # ë³¸ë¬¸
        body = []
        for line in lines[start_pos + 1:end_pos]:
            if line.strip() and not is_header_line(line):
                body.append(line)
        sections[section_name] = "\n".join(body)

    return sections

# â”€â”€ ì‹¤í–‰ë¶€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    # í•„ìš” ê²½ë¡œë¡œ ë°”ê¿”ì„œ ì‚¬ìš©
    pdf_path = r"D:\PROJECT\AI\msds-batch-extractor\msds\msds\test6.pdf"
    # run_debug(pdf_path, section_keys=["ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±", "ë²•ì ê·œì œ"])

    print("=" * 80)
    print("MSDS PDF ì„¹ì…˜ ì¶”ì¶œ (í…ìŠ¤íŠ¸+OCR í•˜ì´ë¸Œë¦¬ë“œ, í˜ì´ì§€ í—¤ë”/í‘¸í„°Â·ëª©ì°¨ ë³´ìˆ˜ ì œê±° + Fallback + í˜ì´ì§€ í—¤ë” ë³´ì¡´)")
    print("=" * 80)
    print(f"\níŒŒì¼ ê²½ë¡œ: {pdf_path}\n")

    if not Path(pdf_path).exists():
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return

    try:
        sections = extract_sections(pdf_path)
        if not sections:
            print("âš ï¸  ê²½ê³ : ì¶”ì¶œëœ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        section_names = {
            "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": "1. í™”í•™ì œí’ˆê³¼ íšŒì‚¬ì— ê´€í•œ ì •ë³´",
            "ìœ í•´ì„±ìœ„í—˜ì„±": "2. ìœ í•´ì„±Â·ìœ„í—˜ì„±",
            "êµ¬ì„±ì„±ë¶„": "3. êµ¬ì„±ì„±ë¶„ì˜ ëª…ì¹­ ë° í•¨ìœ ëŸ‰",
            "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": "9. ë¬¼ë¦¬ í™”í•™ì  íŠ¹ì„±/íŠ¹ì§•",
            "ë²•ì ê·œì œ": "15. ë²•ì  ê·œì œí˜„í™©",
        }

        for key, title in section_names.items():
            if key in sections:
                print("\n" + "=" * 80)
                print(f"ğŸ“‹ {title}")
                print("=" * 80)
                content = sections[key]
                if len(content) > 1200:
                    print(content[:1200])
                    print(f"\n... (ì´ {len(content)}ì, ì¼ë¶€ë§Œ í‘œì‹œ)")
                else:
                    print(content)
            else:
                print(f"\nâš ï¸  {title}: ì°¾ì„ ìˆ˜ ì—†ìŒ")

        print("\n" + "=" * 80)
        print("âœ… ì¶”ì¶œ ì™„ë£Œ")
        print("=" * 80)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()