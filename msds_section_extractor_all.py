import re
from pathlib import Path

import pdfplumber
from difflib import SequenceMatcher
from pdf2image.exceptions import PDFInfoNotInstalledError
# OCR
# pip install pdf2image pytesseract pillow
from pdf2image import convert_from_path
import pytesseract
from PIL import Image

import streamlit as st

POPPLER_PATH = r"C:\Program Files\poppler\poppler-25.07.0\Library\bin"   # ë˜ëŠ” r"C:\Program Files\poppler\bin"
ENABLE_OCR = True     # OCR ì“¸ì§€ ì—¬ë¶€ (Poppler ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ False ì²˜ë¦¬)

# Windows í•„ìš”ì‹œ ê²½ë¡œ ì„¤ì •
# pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

TESS_LANG = "kor+eng"
OCR_DPI = 300
OCR_TEXT_MIN_CHARS = 40  # í˜ì´ì§€ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì´ ê°’ ë¯¸ë§Œì´ë©´ í•´ë‹¹ í˜ì´ì§€ë§Œ OCR

# â”€â”€ ê³µë°±/êµ¬ë¶„ì ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¨ì–´ ì‚¬ì´ êµ¬ë¶„ì: ì¼ë°˜ ê³µë°± + NBSP/ì œë¡œí­ ê³µë°± + êµ¬ë¶„ì ë“¤
sep = r"[\s\u00A0\u2000-\u200B\.\-Â·ãƒ»,ï¼/]*"
# ë²ˆí˜¸ ì ‘ë‘ë¶€/ì‚¬ì´/ë’¤ì— í—ˆìš©í•  ê³µë°± í´ë˜ìŠ¤
WS = r"[\s\u00A0\u2000-\u200B]*"

SECNUM = {
    "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": 1,
    "ìœ í•´ì„±ìœ„í—˜ì„±": 2,
    "êµ¬ì„±ì„±ë¶„": 3,
    "ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹": 4,
    "í­ë°œí™”ì¬ì‹œëŒ€ì²˜ë°©ë²•": 5,
    "ëˆ„ì¶œì‚¬ê³ ì‹œëŒ€ì²˜ë°©ë²•": 6,
    "ì·¨ê¸‰ë°ì €ì¥ë°©ë²•": 7,
    "ë…¸ì¶œë°©ì§€ë°ê°œì¸ë³´í˜¸êµ¬": 8,
    "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": 9,
    "ì•ˆì •ì„±ë°ë°˜ì‘ì„±": 10,
    "ë…ì„±ì—ê´€í•œì •ë³´": 11,
    "í™˜ê²½ì—ë¯¸ì¹˜ëŠ”ì˜í–¥": 12,
    "íê¸°ì‹œì£¼ì˜ì‚¬í•­": 13,
    "ìš´ì†¡ì—í•„ìš”í•œì‚¬í•­": 14,
    "ë²•ì ê·œì œ": 15,
    "ê¸°íƒ€ì°¸ê³ ì‚¬í•­": 16,
}

# ê° ì„¹ì…˜ì—ì„œ "ê°™ì´" ìˆì–´ì•¼ í•˜ëŠ”(ë˜ëŠ” ìˆìœ¼ë©´ ì¢‹ì€) í‚¤ì›Œë“œ ì„¸íŠ¸
PROB_KEYS = {
    1: (["í™”í•™", "ì œí’ˆ", "íšŒì‚¬", "ì •ë³´", "ì œí’ˆëª…"], ["ì œí’ˆ", "íšŒì‚¬", "ì •ë³´"]),
    2: (["ìœ í•´", "ìœ„í—˜"], ["ìœ í•´ì„±", "ìœ„í—˜ì„±", "ìœ í•´ìœ„í—˜"]),
    3: (["êµ¬ì„±", "ì„±ë¶„", "í•¨ëŸ‰", "í•¨ìœ ", "ì¡°ì„±"], ["ì„±ë¶„", "í•¨ëŸ‰", "í•¨ìœ ", "ì¡°ì„±"]),
    4: (["ì‘ê¸‰", "ì¡°ì¹˜"], ["ì‘ê¸‰ì¡°ì¹˜", "ì¡°ì¹˜ìš”ë ¹"]),
    5: (["í­ë°œ", "í™”ì¬"], ["í­ë°œ", "í™”ì¬", "ëŒ€ì²˜"]),
    6: (["ëˆ„ì¶œ", "ì‚¬ê³ "], ["ëˆ„ì¶œ", "ìœ ì¶œ", "ëŒ€ì²˜"]),
    7: (["ì·¨ê¸‰", "ì €ì¥"], ["ì·¨ê¸‰", "ì €ì¥", "ë³´ê´€"]),
    8: (["ë…¸ì¶œ", "ë³´í˜¸êµ¬"], ["ë…¸ì¶œ", "ë°©ì§€", "ê°œì¸", "ë³´í˜¸êµ¬"]),
    9: (["ë¬¼ë¦¬", "í™”í•™", "íŠ¹ì„±", "íŠ¹ì§•"], ["ë¬¼ë¦¬í™”í•™", "íŠ¹ì„±", "íŠ¹ì§•"]),
    10: (["ì•ˆì •", "ë°˜ì‘"], ["ì•ˆì •ì„±", "ë°˜ì‘ì„±"]),
    11: (["ë…ì„±"], ["ë…ì„±", "ë…ì„±ì •ë³´"]),
    12: (["í™˜ê²½"], ["í™˜ê²½", "ì˜í–¥"]),
    13: (["íê¸°"], ["íê¸°", "ì£¼ì˜"]),
    14: (["ìš´ì†¡"], ["ìš´ì†¡", "ì‚¬í•­"]),
    15: (["ë²•ì ", "ë²•ê·œ"], ["ê·œì œ", "ê·œì¡”", "ê·œì œí˜„í™©", "ê·œì¡”í˜„í™©"]),
    # ì—¬ê¸°ë§Œ ìˆ˜ì •
    16: (["ì°¸ê³ ", "ì‚¬í•­"], ["ê¸°íƒ€", "ê·¸ ë°–ì˜", "ì°¸ê³ ì‚¬í•­"]),
}

ALL_SECTION_KEYS = [
    "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´",
    "ìœ í•´ì„±ìœ„í—˜ì„±",
    "êµ¬ì„±ì„±ë¶„",
    "ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹",
    "í­ë°œí™”ì¬ì‹œëŒ€ì²˜ë°©ë²•",
    "ëˆ„ì¶œì‚¬ê³ ì‹œëŒ€ì²˜ë°©ë²•",
    "ì·¨ê¸‰ë°ì €ì¥ë°©ë²•",
    "ë…¸ì¶œë°©ì§€ë°ê°œì¸ë³´í˜¸êµ¬",
    "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±",
    "ì•ˆì •ì„±ë°ë°˜ì‘ì„±",
    "ë…ì„±ì—ê´€í•œì •ë³´",
    "í™˜ê²½ì—ë¯¸ì¹˜ëŠ”ì˜í–¥",
    "íê¸°ì‹œì£¼ì˜ì‚¬í•­",
    "ìš´ì†¡ì—í•„ìš”í•œì‚¬í•­",
    "ë²•ì ê·œì œ",
    "ê¸°íƒ€ì°¸ê³ ì‚¬í•­",
]

SECTION_TITLES = {
    "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": "1. í™”í•™ì œí’ˆê³¼ íšŒì‚¬ì— ê´€í•œ ì •ë³´",
    "ìœ í•´ì„±ìœ„í—˜ì„±": "2. ìœ í•´ì„±Â·ìœ„í—˜ì„±",
    "êµ¬ì„±ì„±ë¶„": "3. êµ¬ì„±ì„±ë¶„ì˜ ëª…ì¹­ ë° í•¨ìœ ëŸ‰",
    "ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹": "4. ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹",
    "í­ë°œí™”ì¬ì‹œëŒ€ì²˜ë°©ë²•": "5. í­ë°œÂ·í™”ì¬ ì‹œ ëŒ€ì²˜ë°©ë²•",
    "ëˆ„ì¶œì‚¬ê³ ì‹œëŒ€ì²˜ë°©ë²•": "6. ëˆ„ì¶œì‚¬ê³  ì‹œ ëŒ€ì²˜ë°©ë²•",
    "ì·¨ê¸‰ë°ì €ì¥ë°©ë²•": "7. ì·¨ê¸‰ ë° ì €ì¥ë°©ë²•",
    "ë…¸ì¶œë°©ì§€ë°ê°œì¸ë³´í˜¸êµ¬": "8. ë…¸ì¶œë°©ì§€ ë° ê°œì¸ë³´í˜¸êµ¬",
    "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": "9. ë¬¼ë¦¬ í™”í•™ì  íŠ¹ì„±/íŠ¹ì§•",
    "ì•ˆì •ì„±ë°ë°˜ì‘ì„±": "10. ì•ˆì •ì„± ë° ë°˜ì‘ì„±",
    "ë…ì„±ì—ê´€í•œì •ë³´": "11. ë…ì„±ì— ê´€í•œ ì •ë³´",
    "í™˜ê²½ì—ë¯¸ì¹˜ëŠ”ì˜í–¥": "12. í™˜ê²½ì— ë¯¸ì¹˜ëŠ” ì˜í–¥",
    "íê¸°ì‹œì£¼ì˜ì‚¬í•­": "13. íê¸° ì‹œ ì£¼ì˜ì‚¬í•­",
    "ìš´ì†¡ì—í•„ìš”í•œì‚¬í•­": "14. ìš´ì†¡ì— í•„ìš”í•œ ì‚¬í•­",
    "ë²•ì ê·œì œ": "15. ë²•ì  ê·œì œí˜„í™©",
    # â† ì œëª©ì„ 'ê·¸ ë°–ì˜ ì°¸ê³ ì‚¬í•­'ìœ¼ë¡œ ë³€ê²½
    "ê¸°íƒ€ì°¸ê³ ì‚¬í•­": "16. ê¸°íƒ€/ê·¸ ë°–ì˜ ì°¸ê³ ì‚¬í•­",
}


def is_probably_section_line(line: str, num: int) -> bool:
    """ì£¼ì–´ì§„ ë¼ì¸ì´ 'ì„¹ì…˜ ë²ˆí˜¸ + í•µì‹¬ í‚¤ì›Œë“œ(AND, í¼ì§€ í—ˆìš©)'ë¥¼ ë§Œì¡±í•˜ë©´ True"""
    s = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
    # 1) ë²ˆí˜¸
    if not re.search(sec(num), s):
        return False
    # 2) í‚¤ì›Œë“œ AND-ish (must ì¤‘ 1ê°œ ì´ìƒ + also ì¤‘ 1ê°œ ì´ìƒ)
    must, also = PROB_KEYS.get(num, ([], []))
    return contains_near(s, must) and contains_near(s, also)


def similar(a, b):
    a = re.sub(r"[\s\u00A0\u2000-\u200B]+", "", a or "")
    b = re.sub(r"[\s\u00A0\u2000-\u200B]+", "", b or "")
    return SequenceMatcher(None, a, b).ratio()


def contains_near(line: str, targets: list[str], threshold=0.78) -> bool:
    """ë¼ì¸ì— targets ì¤‘ í•˜ë‚˜ë¼ë„ ìœ ì‚¬í•˜ê²Œ í¬í•¨ë˜ë©´ True"""
    hay = re.sub(r"\s+", "", line)
    for t in targets:
        if t in hay:
            return True
        # í† í°ì„ ìª¼ê°œì„œ ê·¼ì‚¬ íƒìƒ‰
        for w in re.split(r"[^\wê°€-í£]+", hay):
            if w and similar(w, re.sub(r"\s+", "", t)) >= threshold:
                return True
    return False


def is_probably_legal_section_line(line: str) -> bool:
    """15ë²ˆ ì„¹ì…˜ í—¤ë”ë¥¼ ì˜¤íƒ€ê¹Œì§€ ANDë¡œ ê°ì§€ (ë²ˆí˜¸ + ë²•ì /ë²•ê·œ + ê·œì œ ê³„ì—´)"""
    s = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
    # 1) ë²ˆí˜¸(sec15) ë“¤ì–´ìˆê³ 
    if not re.search(sec(15), s):
        return False
    # 2) 'ë²•ì ' ë˜ëŠ” 'ë²•ê·œ'ë¥¼ ìœ ì‚¬ë„ í—ˆìš©ìœ¼ë¡œ í¬í•¨í•˜ê³ 
    if not contains_near(s, ["ë²•ì ", "ë²•ê·œ"]):
        return False
    # 3) 'ê·œì œ'ë¥¼ ìœ ì‚¬ë„ í—ˆìš©ìœ¼ë¡œ í¬í•¨ (ê·œì¡”, ê·œì œí˜„í™© ë“± ì»¤ë²„)
    if not contains_near(s, ["ê·œì œ", "ê·œì œí˜„í™©", "ê·œì¡”", "ê·œì¡”í˜„í™©"]):
        return False
    return True


def _print_box(title: str):
    print("\n" + "=" * 100)
    print(f"ğŸ” {title}")
    print("=" * 100)


def _show_context(lines, idx, radius=3):
    s = max(0, idx - radius)
    e = min(len(lines), idx + radius + 1)
    for i in range(s, e):
        mark = ">>" if i == idx else "  "
        print(f"{mark} [{i:04d}] {lines[i][:200]}")


def debug_dump_patterns(section_patterns, fallback_rxs):
    _print_box("ì„¹ì…˜ íŒ¨í„´(ë¼ì¸ ê¸°ë°˜) & Fallback(ë©€í‹°ë¼ì¸)")
    for k, pats in section_patterns.items():
        print(f"\n[{k}] ë¼ì¸ ê¸°ë°˜ íŒ¨í„´ {len(pats)}ê°œ")
        for j, p in enumerate(pats, 1):
            print(f"  ({j}) {p}")
        if k in fallback_rxs:
            print(f"  â†³ Fallback: {fallback_rxs[k].pattern}")


def debug_try_line_match(lines, pats, title="(ë¼ì¸ ê¸°ë°˜ ì •ê·œì‹)"):
    hit_idxs = []
    for i, line in enumerate(lines):
        line_cmp = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
        for p in pats:
            if re.search(p, line_cmp, re.IGNORECASE):
                hit_idxs.append(i)
                break
    print(f"  - {title} ë§¤ì¹˜ ë¼ì¸ ìˆ˜: {len(hit_idxs)}")
    if hit_idxs:
        print("  - ì²« 3ê°œ í›„ë³´:")
        for i in hit_idxs[:3]:
            _show_context(lines, i, radius=1)
    return hit_idxs


def debug_try_number_only(lines, n):
    print(f"  - ë²ˆí˜¸í—¤ë” sec({n})ë§Œ ë§¤ì¹­ë˜ëŠ” ë¼ì¸(ì˜¤íƒ ê°€ëŠ¥) ì²´í¬")
    rx = re.compile(sec(n), re.IGNORECASE)
    hits = [i for i, ln in enumerate(lines) if rx.search(re.sub(r"[\u00A0\u2000-\u200B]", " ", ln))]
    print(f"    Â· ë§¤ì¹˜ {len(hits)}ê°œ")
    for i in hits[:3]:
        _show_context(lines, i, 1)
    return hits


def debug_try_keyword_only(lines, keyword_regex, title="í‚¤ì›Œë“œë§Œ"):
    print(f"  - {title} ë§¤ì¹­ ë¼ì¸(ë²ˆí˜¸ ì—†ì´ í‚¤ì›Œë“œë§Œ ìˆëŠ” ì¤„) ì²´í¬")
    rx = re.compile(keyword_regex, re.IGNORECASE)
    hits = [i for i, ln in enumerate(lines) if rx.search(re.sub(r"[\u00A0\u2000-\u200B]", " ", ln))]
    print(f"    Â· ë§¤ì¹˜ {len(hits)}ê°œ")
    for i in hits[:3]:
        _show_context(lines, i, 1)
    return hits


def debug_try_fallback(full_text, rx, lines, title="Fallback"):
    print(f"  - {title} ë©€í‹°ë¼ì¸ ê²€ìƒ‰")
    txt = re.sub(r"[\u00A0\u2000-\u200B]", " ", full_text)
    m = rx.search(txt)
    if not m:
        print("    Â· ë§¤ì¹˜ ì—†ìŒ")
        return -1
    idx = txt[:m.start()].count("\n")
    print(f"    Â· ë§¤ì¹˜ ì‹œì‘ ì¤„ index = {idx}")
    _show_context(lines, idx, 2)
    return idx


def debug_next_boundary(lines, start_idx, next_num):
    print(f"  - ë‹¤ìŒ ë²ˆí˜¸ ê²½ê³„ íƒìƒ‰: {next_num}")
    end_idx = find_next_boundary_for(lines, start_idx, next_num)
    if end_idx == len(lines):
        print("    Â· ë‹¤ìŒ ë²ˆí˜¸ ê²½ê³„ ë¯¸ë°œê²¬(ë¬¸ì„œ ëê¹Œì§€)")
    else:
        print(f"    Â· ê²½ê³„ ë¼ì¸ index = {end_idx}")
        _show_context(lines, end_idx, 1)
    return end_idx


def debug_toc_pages(pdf_path: str):
    print("\n" + "-" * 60)
    print("ğŸ“„ í˜ì´ì§€ë³„ TOC(ëª©ì°¨) íŒì • ìš”ì•½")
    print("-" * 60)
    with pdfplumber.open(pdf_path) as pdf:
        for pi, page in enumerate(pdf.pages, 1):
            t = page.extract_text() or ""
            t = strip_page_edges(t)
            flag = is_toc_page(t)
            print(f"  p{pi:02d}  TOC={flag}   (chars={len(t)})")
            if flag:
                # ëª©ì°¨ë¡œ ë³¸ ê²½ìš° ì• ëª‡ ì¤„ë§Œ ë³´ì—¬ì£¼ê¸°
                lines = [ln for ln in t.split("\n") if ln.strip()]
                for ln in lines[:5]:
                    print("     Â·", ln[:200])


def run_debug(pdf_path: str, section_keys=None):
    """
    section_keys: ["ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±","ë²•ì ê·œì œ"] ë“±. Noneì´ë©´ 1/2/3/9/15 ëª¨ë‘
    """
    if section_keys is None:
        section_keys = ["í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´", "ìœ í•´ì„±ìœ„í—˜ì„±", "êµ¬ì„±ì„±ë¶„", "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±", "ë²•ì ê·œì œ"]

    # 0) í˜ì´ì§€ë³„ TOC íŒì • ì°¸ê³  (ëª©ì°¨ë¡œ ì œê±°ë˜ëŠ”ì§€ ì‹œê°í™”)
    debug_toc_pages(pdf_path)

    # 1) ì›ë¬¸ í…ìŠ¤íŠ¸/í´ë¦° í…ìŠ¤íŠ¸ í™•ë³´
    page_texts = extract_text_pages_hybrid(pdf_path)
    full_raw = "\n".join(page_texts)             # strip_toc_block ì ìš© ì „
    lines_raw = full_raw.split("\n")

    lines = remove_repeated_headers(lines_raw)
    lines = strip_toc_block(lines)
    full_clean = "\n".join(lines)

    # 2) íŒ¨í„´ ë¤í”„
    section_patterns = find_section_patterns()
    debug_dump_patterns(section_patterns, FALLBACK_HEAD_RXS)

    # 3) ì„¹ì…˜ë³„ ë””ë²„ê·¸
    for key in section_keys:
        _print_box(f"ì„¹ì…˜ ë””ë²„ê¹…: {key}")
        pats = section_patterns[key]

        print(" (A) ë¼ì¸ ê¸°ë°˜: í´ë¦°í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œì‹ íƒìƒ‰")
        hit_idxs = debug_try_line_match(lines, pats)

        # ë²ˆí˜¸ë§Œ / í‚¤ì›Œë“œë§Œ ì²´í¬(ì˜¤íƒ/ëˆ„ë½ ìœ í˜• íŒŒì•…)
        if key == "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±":
            debug_try_number_only(lines, 9)
            debug_try_keyword_only(lines, r"(ë¬¼ë¦¬\s*í™”í•™\s*ì |ë¬¼ë¦¬\s*í™”í•™|ë¬¼ë¦¬\s*ì )\s*(íŠ¹ì„±|íŠ¹ì§•)", "ë¬¼ë¦¬/í™”í•™ í‚¤ì›Œë“œ")
        elif key == "ë²•ì ê·œì œ":
            debug_try_number_only(lines, 15)
            debug_try_keyword_only(lines, r"(ë²•ì |ë²•ê·œ)\s*ê·œì œ(\s*í˜„í™©)?", "ë²•ì /ê·œì œ í‚¤ì›Œë“œ")

        # (B) Fallback: ì›ë¬¸ -> í´ë¦° ìˆœì„œë¡œ ì‹œë„
        print(" (B) ë©€í‹°ë¼ì¸ Fallback: ì›ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰")
        fb_idx_raw = debug_try_fallback(full_raw, FALLBACK_HEAD_RXS[key], lines_raw, "Fallback(raw)")

        print(" (C) ë©€í‹°ë¼ì¸ Fallback: í´ë¦° í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰")
        fb_idx_clean = debug_try_fallback(full_clean, FALLBACK_HEAD_RXS[key], lines, "Fallback(clean)")

        # (D) ê²½ê³„ í™•ì¸(ì‹œì‘ í›„ë³´ê°€ ìˆì„ ë•Œë§Œ)
        start_idx = None
        if hit_idxs:
            start_idx = hit_idxs[0]
        elif fb_idx_clean != -1:
            start_idx = fb_idx_clean
        elif fb_idx_raw != -1:
            # raw ê¸°ì¤€ ì¤„ ë²ˆí˜¸ë¥¼ clean ê¸°ì¤€ìœ¼ë¡œ ê·¼ì‚¬ ë§¤í•‘(ì™„ë²½í•˜ì§„ ì•Šì§€ë§Œ ë§¥ë½ í™•ì¸ìš©)
            start_idx = min(len(lines) - 1, fb_idx_raw)

        if start_idx is not None:
            if key in BOUNDARY_NEXT_NUMBER:
                debug_next_boundary(lines, start_idx, BOUNDARY_NEXT_NUMBER[key])
            else:
                print("  - ê²½ê³„ íƒìƒ‰ ì—†ìŒ(íƒ€ê¹ƒ ì„¹ì…˜ ì•„ë‹˜)")
        else:
            print("  - ì‹œì‘ í›„ë³´ ìì²´ê°€ ì—†ì–´ ê²½ê³„ íƒìƒ‰ ìƒëµ")


def sec(n: int) -> str:
    """
    í–‰ ì‹œì‘ ë²ˆí˜¸ í‘œê¸° í—ˆìš©:
    - [9], 9., 9), 9-, 9:, 9 (êµ¬ë¶„ì ì—†ì´ ê³µë°±ë§Œ)
    - ì „ê° ì /ì½œë¡ /ì¼ë³¸ì–´ ë§ˆì¹¨í‘œ í—ˆìš©: ï¼ ï¼š ã€‚
    - ì œ 9 ì¥/í•­
    - ì¶”ê°€: ìˆ«ì ì•ì˜ ë°±í‹±/ë”°ì˜´í‘œ/ë¶ˆë¦¿ ë“± ì¡ë¬¸ì í—ˆìš©
    """
    punc = r"[\.\)\-:ï¼šï¼ã€‚]"
    lead = r"[\s\u00A0\u2000-\u200B`\uFEFF\"'â€œâ€â€˜â€™Â·â€¢â€“â€”-]*"
    return rf"^{lead}(?:\[?{n}\]?|{n}{WS}(?:{punc})?{WS}|ì œ?{WS}{n}{WS}[ì¥í•­]){WS}"


def normalize_text(text: str) -> str:
    return re.sub(r"\s+", "", (text or "").lower())


# â”€â”€ ì „ì—­ ë°˜ë³µ í—¤ë”/í‘¸í„°(ë¬¸ì„œ ì „ë°˜ì—ì„œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_header_line(line: str) -> bool:
    """
    í˜ì´ì§€ ìƒë‹¨/í•˜ë‹¨ì˜ ë°˜ë³µ í—¤ë”/í‘¸í„°ë¥¼ íŒì •.
    ì¼ë°˜ ë³¸ë¬¸ê¹Œì§€ ì˜ë¦¬ì§€ ì•Šë„ë¡ íŒ¨í„´ì„ ìµœëŒ€í•œ ë³´ìˆ˜ì ìœ¼ë¡œ ë‘”ë‹¤.
    """
    normalized = normalize_text(line)

    # 'ë³¸msdsëŠ” ...' ì²˜ëŸ¼ ë³¸ë¬¸ì— ìì£¼ ë“±ì¥í•˜ëŠ” ë¬¸ì¥ì€ ì ˆëŒ€ í—¤ë”ë¡œ ë³´ì§€ ì•ŠìŒ
    if "ë³¸msdsëŠ”" in normalized:
        return False

    header_patterns = [
        r"msdsë²ˆí˜¸",
        r"ë¬¸ì„œë²ˆí˜¸",
        r"ê°œì •ì¼ì",
        r"ê°œì •ë²ˆí˜¸",

        # ì œëª© í•œ ì¤„ ê·¸ëŒ€ë¡œì¸ ê²½ìš°ë§Œ í—¤ë”ë¡œ ê°„ì£¼ (ë³¸ë¬¸ì— ì„ì¸ 'ë¬¼ì§ˆì•ˆì „ë³´ê±´ìë£Œì— ê´€í•œ ê¸°ì¤€' ë“±ì€ ì œì™¸)
        r"^ë¬¼ì§ˆì•ˆì „ë³´ê±´ìë£Œ$",
        r"^materialsafetydatasheets?$",

        r"ghs[\-\s]?msds",

        # í˜ì´ì§€ ë²ˆí˜¸
        r"\d+\s*/\s*\d+\s*(í˜ì´ì§€|page)",
        r"page\s*\d+\s*/\s*\d+",

        # ê°œì • ë²„ì „/ì €ì‘ê¶Œ
        r"-\d+/\d+-\s*rev\.",
        r"rev\.\s*\d+",
        r"copyright",
        r"all\s*rights\s*reserved",
    ]
    return any(re.search(p, normalized) for p in header_patterns)


def remove_repeated_headers(lines):
    """ë¬¸ì„œ ì•ë¶€ë¶„ì—ì„œ ê°ì§€ëœ ë°˜ë³µ ë¼ì¸ì„ ì „ì²´ì—ì„œ ì œê±°"""
    if not lines:
        return lines
    header_lines = set()
    for line in lines[:10]:
        if is_header_line(line):
            header_lines.add(normalize_text(line))
    return [ln for ln in lines if normalize_text(ln) not in header_lines]


# â”€â”€ í˜ì´ì§€ ê°€ì¥ìë¦¬(ìƒÂ·í•˜ë‹¨) ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PAGE_MARK_RE = re.compile(r"\b\d+\s*/\s*\d+\s*(?:í˜ì´ì§€|page)\b", re.IGNORECASE)
DOC_MARK_RE = re.compile(r"ghs[\-\s]?msds", re.IGNORECASE)


def strip_page_edges(text: str) -> str:
    """ê° í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ì²« 3ì¤„/ë§ˆì§€ë§‰ 3ì¤„ì˜ í—¤ë”Â·í‘¸í„° ì œê±°"""
    lines = text.split("\n") if text else []
    if not lines:
        return text
    new = []
    for i, ln in enumerate(lines):
        at_top = i < 3
        at_bot = i >= len(lines) - 3
        if (at_top and (DOC_MARK_RE.search(ln) or is_header_line(ln))) \
                or (at_bot and (PAGE_MARK_RE.search(ln) or is_header_line(ln))):
            continue
        new.append(ln)
    return "\n".join(new)


# â”€â”€ ëª©ì°¨(TOC) ê°ì§€/ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOC_HINT_WORDS = {
    "ëª©ì°¨", "contents", "table of contents", "ghs-msds", "ë¬¼ì§ˆ ì•ˆì „ë³´ê±´ìë£Œ",
}
TOC_SECTION_KEYS = {
    "í™”í•™", "íšŒì‚¬", "ìœ í•´", "ìœ„í—˜", "êµ¬ì„±", "ì‘ê¸‰", "í­ë°œ", "ëˆ„ì¶œ",
    "ì·¨ê¸‰", "ë³´ê´€", "ë…¸ì¶œ", "ë³´í˜¸êµ¬", "ë¬¼ë¦¬", "í™”í•™ì ", "ì•ˆì •ì„±", "ë°˜ì‘ì„±",
    "ë…ì„±", "í™˜ê²½", "íê¸°", "ìš´ì†¡", "ë²•ì ", "ê·œì œ", "ê¸°íƒ€", "ì°¸ê³ "
}


def is_toc_like_numbering(line: str) -> int:
    """ëª©ì°¨ ìˆ«ì ë¼ì¸: '1. â€¦', '10) â€¦', '[15] â€¦' í˜•íƒœë©´ ë²ˆí˜¸ ë°˜í™˜, ì•„ë‹ˆë©´ -1"""
    m = re.match(r"^\s*(?:\[(\d{1,2})\]|(\d{1,2})\s*[\.\):])", line)
    if not m:
        return -1
    n = m.group(1) or m.group(2)
    try:
        val = int(n)
        return val if 1 <= val <= 16 else -1
    except Exception:
        return -1


def is_toc_page(text: str) -> bool:
    """í˜ì´ì§€ ì „ì²´ê°€ ëª©ì°¨/í‘œì§€ë¡œ ë³´ì´ë©´ True (ì¡°ê¸ˆ ë” ê°•í•˜ê²Œ)"""
    if not text:
        return False
    t = text.strip()
    lines = [ln for ln in t.split("\n") if ln.strip()]

    # íŒíŠ¸ ë‹¨ì–´
    hint = any(h.lower() in t.lower() for h in TOC_HINT_WORDS)

    # ë²ˆí˜¸ í˜•íƒœ(1., 10), [15])ê°€ ì—¬ëŸ¬ ê°œ ë‚˜ì˜¤ë©´ ëª©ì°¨ì— ê°€ê¹Œì›€
    nums = []
    numbered_lines = 0
    for ln in lines:
        n = is_toc_like_numbering(ln)
        if n != -1:
            nums.append(n)
            numbered_lines += 1
    unique_nums = set(nums)

    # ì„¹ì…˜ í‚¤ì›Œë“œ ë‹¤ìˆ˜
    kw_hits = sum(any(kw in ln for kw in TOC_SECTION_KEYS) for ln in lines)
    kw_ratio = kw_hits / max(1, len(lines))

    # ê°•í™”ëœ ê¸°ì¤€
    return (
        hint
        or (
            len(unique_nums) >= 6
            and (max(unique_nums, default=0) <= 16)
            and (numbered_lines / max(1, len(lines)) >= 0.30)
            and kw_ratio >= 0.10
        )
    )


# â”€â”€ ì„¹ì…˜ íŒ¨í„´(ëŠìŠ¨í•˜ê²Œ ë³´ê°•) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_section_patterns():
    return {
        "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": [
            sec(1) + rf"í™”í•™{sep}ì œí’ˆ{sep}ê³¼{sep}íšŒì‚¬(?:{sep}ì—{sep}ê´€í•œ{sep}ì •ë³´)?",
            sec(1) + rf"í™”í•™{sep}ì œí’ˆ",
            sec(1) + rf"ì œí’ˆ{sep}ëª…",
            sec(1) + rf"í™”í•™{sep}íšŒì‚¬",
        ],
        "ìœ í•´ì„±ìœ„í—˜ì„±": [
            sec(2) + rf"ìœ í•´{sep}ì„±{sep}[Â·ãƒ»\.]?{sep}ìœ„í—˜{sep}ì„±",
            sec(2) + rf"ìœ í•´{sep}ìœ„í—˜{sep}ì„±",
            sec(2) + rf"ìœ í•´{sep}ì„±",
            sec(2) + rf"ìœ í•´{sep}ìœ„í—˜",
            sec(2) + rf"ìœ„í—˜{sep}ì„±{sep}[Â·ãƒ»\.]?{sep}ìœ í•´{sep}ì„±",
            sec(2) + rf"ìœ„í—˜{sep}ìœ í•´{sep}ì„±",
            sec(2) + rf"ìœ„í—˜{sep}ìœ í•´",
            sec(2) + rf"(?:ìœ í•´|ìœ„í—˜){sep}ì„±{sep}ë°{sep}(?:ìœ í•´|ìœ„í—˜){sep}ì„±",
            sec(2) + rf"(?:ìœ í•´|ìœ„í—˜){sep}ë°{sep}(?:ìœ í•´|ìœ„í—˜){sep}ì„±",
        ],
        "êµ¬ì„±ì„±ë¶„": [
            sec(3) + rf"êµ¬ì„±{sep}ì„±ë¶„(?:{sep}ì˜{sep}ëª…ì¹­{sep}ë°{sep}(?:í•¨ìœ ?{sep}?ëŸ‰|í•¨ëŸ‰|ì¡°ì„±))?",
            sec(3) + rf"(?:êµ¬ì„±{sep})?ì„±ë¶„{sep}(?:í‘œ|ì •ë³´)?",
            sec(3) + rf"ì„±ë¶„{sep}(?:ëª…|ëª…ì¹­){sep}ë°{sep}(?:í•¨ìœ ?{sep}?ëŸ‰|í•¨ëŸ‰)",
            sec(3) + rf"ì¡°ì„±{sep}(?:ë°{sep}ëª…ì¹­|ì •ë³´|í‘œ)?",
        ],
        "ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹": [
            sec(4) + rf"ì‘ê¸‰{sep}ì¡°ì¹˜{sep}(?:ìš”ë ¹|ë°©ë²•)?",
            sec(4) + rf"ì‘ê¸‰{sep}ì¡°ì¹˜",
        ],
        "í­ë°œí™”ì¬ì‹œëŒ€ì²˜ë°©ë²•": [
            sec(5) + rf"(?:í­ë°œ|í™”ì¬){sep}ì‹œ{sep}(?:ëŒ€ì²˜|ì¡°ì¹˜){sep}ë°©ë²•?",
            sec(5) + rf"í™”ì¬{sep}ë°{sep}í­ë°œ{sep}ì‹œ{sep}(?:ëŒ€ì²˜|ì¡°ì¹˜)",
        ],
        "ëˆ„ì¶œì‚¬ê³ ì‹œëŒ€ì²˜ë°©ë²•": [
            sec(6) + rf"(?:ëˆ„ì¶œ|ìœ ì¶œ){sep}ì‚¬ê³ {sep}ì‹œ{sep}(?:ëŒ€ì²˜|ì¡°ì¹˜){sep}ë°©ë²•?",
            sec(6) + rf"(?:ëˆ„ì¶œ|ìœ ì¶œ){sep}(?:ëŒ€ì²˜|ì¡°ì¹˜)",
        ],
        "ì·¨ê¸‰ë°ì €ì¥ë°©ë²•": [
            sec(7) + rf"ì·¨ê¸‰{sep}ë°{sep}ì €ì¥{sep}ë°©ë²•",
            sec(7) + rf"ì·¨ê¸‰{sep}ë°{sep}ë³´ê´€",
        ],
        "ë…¸ì¶œë°©ì§€ë°ê°œì¸ë³´í˜¸êµ¬": [
            sec(8) + rf"ë…¸ì¶œ{sep}ë°©ì§€{sep}ë°{sep}ê°œì¸{sep}ë³´í˜¸êµ¬",
            sec(8) + rf"ë…¸ì¶œ{sep}ë°©ì§€",
            sec(8) + rf"ê°œì¸{sep}ë³´í˜¸êµ¬",
        ],
        "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": [
            sec(9) + rf"ë¬¼ë¦¬{sep}í™”í•™{sep}?ì {sep}(?:íŠ¹ì„±|íŠ¹ì§•)",
            sec(9) + rf"ë¬¼ë¦¬{sep}í™”í•™{sep}(?:íŠ¹ì„±|íŠ¹ì§•)",
            sec(9) + rf"ë¬¼ë¦¬{sep}ì {sep}(?:íŠ¹ì„±|íŠ¹ì§•)",
        ],
        "ì•ˆì •ì„±ë°ë°˜ì‘ì„±": [
            sec(10) + rf"ì•ˆì •{sep}ì„±{sep}ë°{sep}ë°˜ì‘{sep}ì„±",
            sec(10) + rf"ì•ˆì •{sep}ì„±",
            sec(10) + rf"ë°˜ì‘{sep}ì„±",
        ],
        "ë…ì„±ì—ê´€í•œì •ë³´": [
            sec(11) + rf"ë…ì„±{sep}ì—{sep}ê´€í•œ{sep}ì •ë³´",
            sec(11) + rf"ë…ì„±{sep}ì •ë³´",
        ],
        "í™˜ê²½ì—ë¯¸ì¹˜ëŠ”ì˜í–¥": [
            sec(12) + rf"í™˜ê²½{sep}ì—{sep}ë¯¸ì¹˜ëŠ”{sep}ì˜í–¥",
            sec(12) + rf"í™˜ê²½{sep}ì˜í–¥",
        ],
        "íê¸°ì‹œì£¼ì˜ì‚¬í•­": [
            sec(13) + rf"íê¸°{sep}ì‹œ{sep}ì£¼ì˜{sep}ì‚¬í•­",
            sec(13) + rf"íê¸°{sep}ë°©ë²•",
        ],
        "ìš´ì†¡ì—í•„ìš”í•œì‚¬í•­": [
            sec(14) + rf"ìš´ì†¡{sep}ì—{sep}í•„ìš”í•œ{sep}ì‚¬í•­",
            sec(14) + rf"ìš´ì†¡{sep}ì—{sep}ê´€í•œ{sep}ì‚¬í•­",
        ],
        "ë²•ì ê·œì œ": [
            sec(15) + rf"(?:ë²•ì |ë²•\s*ê·œ){sep}ê·œ[ì œì¡”](?:{sep}í˜„í™©)?",
            sec(15) + rf"(?:ê´€ë ¨|ê¸°\s*íƒ€)?{sep}(?:ë²•|ê·œ){sep}ì œ",
            sec(15) + rf"(?:ë²•ì |ë²•\s*ê·œ){sep}ê·œì¡”(?:{sep}í˜„í™©)?",
        ],
        "ê¸°íƒ€ì°¸ê³ ì‚¬í•­": [
            sec(16) + rf"ê¸°íƒ€{sep}ì°¸ê³ {sep}ì‚¬í•­",
            sec(16) + rf"ê¸°íƒ€{sep}ì‚¬í•­",
            sec(16) + rf"ê·¸{sep}ë°–{sep}ì˜{sep}ì°¸ê³ {sep}ì‚¬í•­",
            sec(16) + rf"ê·¸{sep}ë°–{sep}ì˜{sep}ì‚¬í•­",
        ],
    }


# â”€â”€ ìœ ì‚¬ë„(ë°±ì—…) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FUZZY_CANDIDATES = {
    "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": ["í™”í•™ ì œí’ˆê³¼ íšŒì‚¬", "í™”í•™ì œí’ˆ", "í™”í•™ íšŒì‚¬", "íšŒì‚¬ ì •ë³´"],
    "ìœ í•´ì„±ìœ„í—˜ì„±": ["ìœ í•´ ìœ„í—˜ì„±", "ìœ„í—˜ ìœ í•´ì„±", "ìœ í•´ì„±", "ìœ„í—˜ì„±", "ìœ í•´ ìœ„í—˜"],
    "êµ¬ì„±ì„±ë¶„": ["êµ¬ì„± ì„±ë¶„", "ì„±ë¶„í‘œ", "ì„±ë¶„ í•¨ìœ ëŸ‰", "ì„±ë¶„ í•¨ëŸ‰", "ì¡°ì„± ì„±ë¶„"],
    "ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹": ["ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹", "ì‘ê¸‰ ì¡°ì¹˜ ìš”ë ¹", "ì‘ê¸‰ì¡°ì¹˜", "ì‘ê¸‰ ì¡°ì¹˜"],
    "í­ë°œí™”ì¬ì‹œëŒ€ì²˜ë°©ë²•": ["í­ë°œ í™”ì¬ ì‹œ ëŒ€ì²˜ë°©ë²•", "í­ë°œ ë° í™”ì¬ì‹œ ëŒ€ì²˜ë°©ë²•", "í­ë°œ í™”ì¬ ëŒ€ì²˜", "í™”ì¬ í­ë°œ ì¡°ì¹˜"],
    "ëˆ„ì¶œì‚¬ê³ ì‹œëŒ€ì²˜ë°©ë²•": ["ëˆ„ì¶œì‚¬ê³ ì‹œ ëŒ€ì²˜ë°©ë²•", "ëˆ„ì¶œ ì‚¬ê³  ëŒ€ì²˜", "ìœ ì¶œì‚¬ê³  ëŒ€ì²˜"],
    "ì·¨ê¸‰ë°ì €ì¥ë°©ë²•": ["ì·¨ê¸‰ ë° ì €ì¥ë°©ë²•", "ì·¨ê¸‰ ë° ë³´ê´€", "ì €ì¥ë°©ë²•"],
    "ë…¸ì¶œë°©ì§€ë°ê°œì¸ë³´í˜¸êµ¬": ["ë…¸ì¶œë°©ì§€ ë° ê°œì¸ë³´í˜¸êµ¬", "ë…¸ì¶œ ë°©ì§€", "ê°œì¸ ë³´í˜¸êµ¬"],
    "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": ["ë¬¼ë¦¬ í™”í•™ì  íŠ¹ì„±", "ë¬¼ë¦¬ í™”í•™ì  íŠ¹ì§•", "ë¬¼ë¦¬. í™”í•™ì  íŠ¹ì„±", "ë¬¼ë¦¬Â·í™”í•™ì  íŠ¹ì„±"],
    "ì•ˆì •ì„±ë°ë°˜ì‘ì„±": ["ì•ˆì •ì„± ë° ë°˜ì‘ì„±", "ì•ˆì •ì„±", "ë°˜ì‘ì„±"],
    "ë…ì„±ì—ê´€í•œì •ë³´": ["ë…ì„±ì— ê´€í•œ ì •ë³´", "ë…ì„± ì •ë³´", "ë…ì„±"],
    "í™˜ê²½ì—ë¯¸ì¹˜ëŠ”ì˜í–¥": ["í™˜ê²½ì— ë¯¸ì¹˜ëŠ” ì˜í–¥", "í™˜ê²½ ì˜í–¥"],
    "íê¸°ì‹œì£¼ì˜ì‚¬í•­": ["íê¸°ì‹œ ì£¼ì˜ì‚¬í•­", "íê¸° ì‹œ ì£¼ì˜ì‚¬í•­", "íê¸° ë°©ë²•"],
    "ìš´ì†¡ì—í•„ìš”í•œì‚¬í•­": ["ìš´ì†¡ì— í•„ìš”í•œ ì‚¬í•­", "ìš´ì†¡ì— ê´€í•œ ì‚¬í•­", "ìš´ì†¡ ì‚¬í•­"],
    "ë²•ì ê·œì œ": ["ë²•ì  ê·œì œ", "ë²•ì  ê·œì œ í˜„í™©", "ë²•ê·œ ê·œì œ", "ë²•ê·œ ê·œì œ í˜„í™©"],
    "ê¸°íƒ€ì°¸ê³ ì‚¬í•­": [
        "ê¸°íƒ€ ì°¸ê³ ì‚¬í•­",
        "ê¸°íƒ€ ì‚¬í•­",
        "ê¸°íƒ€ ì°¸ê³ ",
        "ê·¸ ë°–ì˜ ì°¸ê³ ì‚¬í•­",
        "ê·¸ ë°–ì˜ ì‚¬í•­",
    ],
}


def is_product_name_line(s: str) -> bool:
    """
    ì„¹ì…˜ 1 ë³¸ë¬¸ ì²« í•­ëª© 'ì œí’ˆëª…' ë¼ì¸ ê°ì§€.
    - '1)ì œí’ˆëª…', 'ì œí’ˆëª… :', 'Product name' ë“± í—ˆìš©
    """
    if not s:
        return False
    line = re.sub(r"[\u00A0\u2000-\u200B]", " ", s).strip()
    # ë²ˆí˜¸ ì ‘ë‘ í—ˆìš©: 1), 1., [1] ë“±ì€ ì„ íƒì 
    if re.match(r"^\s*(?:\[(?:1|â‘ )\]|1\s*[\.\):]?)?\s*(ì œí’ˆ\s*ëª…|ì œí’ˆëª…|product\s*name)\s*[:ï¼š]?", line, re.IGNORECASE):
        return True
    return False


def looks_like_sentence(line: str) -> bool:
    s = re.sub(r"[\u00A0\u2000-\u200B]", " ", line).strip()
    bad_phrases = ["ì—ëŠ”", "ì—ëŠ” ", "ì— ", "ì°¸ì¡°", "ì•„ë˜ í‘œ", "ì•„ë˜í‘œ", "ì•„ë˜ ê¸°ì¬", "ì•„ë˜ì—", "ë³´ê¸°"]
    if any(p in s for p in bad_phrases):
        return True
    if re.search(r"[\.ã€‚ï¼:ï¼š]$", s):
        return True
    return False


def find_all_section_starts(lines, patterns, section_key=None):
    idxs = []
    for i, line in enumerate(lines):
        line_cmp = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
        for pattern in patterns:
            if re.search(pattern, line_cmp, re.IGNORECASE):
                if section_key == "êµ¬ì„±ì„±ë¶„" and looks_like_sentence(line_cmp):
                    continue
                idxs.append(i)
                break
    return idxs


def count_body_lines_between(lines, start_idx, end_idx):
    """í—¤ë”/ë¹ˆì¤„ ì œì™¸ ì‹¤ë‚´ìš© ë¼ì¸ìˆ˜ë¥¼ ìƒŒë‹¤"""
    cnt = 0
    for line in lines[start_idx + 1:end_idx]:
        if line.strip() and not is_header_line(line):
            cnt += 1
    return cnt


def has_composition_table_header_ahead(lines, start_idx, lookahead=20):
    hay = "\n".join(lines[start_idx + 1: min(len(lines), start_idx + 1 + lookahead)])
    hay = re.sub(r"[\u00A0\u2000-\u200B]", " ", hay)
    return any(k in hay for k in ["í™”í•™ë¬¼ì§ˆëª…", "ì¹´ìŠ¤", "CAS", "í•¨ìœ ëŸ‰", "ì„±ë¶„í‘œ"])


def select_best_start(lines, candidate_idxs, section_name):
    if not candidate_idxs:
        return -1
    best_idx = candidate_idxs[-1]
    best_score = -1

    for s in candidate_idxs:
        if section_name in BOUNDARY_NEXT_NUMBER:
            forced_end = find_next_boundary_for(lines, s, BOUNDARY_NEXT_NUMBER[section_name])
        else:
            later = [c for c in candidate_idxs if c > s]
            forced_end = (min(later) if later else len(lines))

        body_cnt = count_body_lines_between(lines, s, forced_end)
        score = body_cnt

        if section_name == "êµ¬ì„±ì„±ë¶„":
            if has_composition_table_header_ahead(lines, s, 20):
                score += 50
            if looks_like_sentence(lines[s]):
                score -= 30

        if (score > best_score) or (score == best_score and s > best_idx):
            best_score = score
            best_idx = s

    if best_score <= 1:
        best_idx = candidate_idxs[-1]

    return best_idx


def fuzzy_find_section_line(lines, candidates, threshold=0.78):
    best_idx, best_score = -1, 0.0
    for i, line in enumerate(lines):
        line_clean = re.sub(r"[\s\u00A0\u2000-\u200B]+", "", line)
        for cand in candidates:
            cand_clean = re.sub(r"[\s\u00A0\u2000-\u200B]+", "", cand)
            score = SequenceMatcher(None, line_clean, cand_clean).ratio()
            if score > best_score:
                best_idx, best_score = i, score
    return best_idx if best_score >= threshold else -1


def find_section_start(lines, patterns, section_key=None):
    candidates = find_all_section_starts(lines, patterns, section_key=section_key)

    if not candidates and section_key and section_key in SECNUM:
        secnum = SECNUM[section_key]
        for i, ln in enumerate(lines):
            if is_probably_section_line(ln, secnum):
                candidates.append(i)

    if not candidates and section_key and section_key in FUZZY_CANDIDATES:
        idx = fuzzy_find_section_line(
            [re.sub(r"[\s\u00A0\u2000-\u200B]+", "", ln) for ln in lines],
            FUZZY_CANDIDATES[section_key]
        )
        return idx

    return select_best_start(lines, candidates, section_key if section_key else "")


# â”€â”€ ì •í™• ê²½ê³„: 3â†’4, 9â†’10, 15â†’16 ë“± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BOUNDARY_NEXT_NUMBER = {
    "êµ¬ì„±ì„±ë¶„": 4,
    "ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹": 5,
    "í­ë°œí™”ì¬ì‹œëŒ€ì²˜ë°©ë²•": 6,
    "ëˆ„ì¶œì‚¬ê³ ì‹œëŒ€ì²˜ë°©ë²•": 7,
    "ì·¨ê¸‰ë°ì €ì¥ë°©ë²•": 8,
    "ë…¸ì¶œë°©ì§€ë°ê°œì¸ë³´í˜¸êµ¬": 9,
    "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": 10,
    "ì•ˆì •ì„±ë°ë°˜ì‘ì„±": 11,
    "ë…ì„±ì—ê´€í•œì •ë³´": 12,
    "í™˜ê²½ì—ë¯¸ì¹˜ëŠ”ì˜í–¥": 13,
    "íê¸°ì‹œì£¼ì˜ì‚¬í•­": 14,
    "ìš´ì†¡ì—í•„ìš”í•œì‚¬í•­": 15,
    "ë²•ì ê·œì œ": 16,
    # 16ë²ˆ(ê¸°íƒ€ì°¸ê³ ì‚¬í•­)ì€ ë‹¤ìŒ ë²ˆí˜¸ê°€ ì—†ìœ¼ë¯€ë¡œ ê²½ê³„ ì—†ìŒ
}


def head_only(n: int) -> re.Pattern:
    return re.compile(sec(n) + r".*$", re.IGNORECASE)


def find_next_boundary_for(lines, start_idx, next_num):
    pat = head_only(next_num)
    for i in range(start_idx + 1, len(lines)):
        if pat.search(re.sub(r"[\u00A0\u2000-\u200B]", " ", lines[i])):
            return i
    return len(lines)


# â”€â”€ í˜ì´ì§€ì— ì„¹ì…˜ í—¤ë”ê°€ ìˆìœ¼ë©´ ì ˆëŒ€ ë²„ë¦¬ì§€ ì•Šê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def page_contains_section_head(text: str) -> bool:
    if not text:
        return False
    hay = re.sub(r"[\u00A0\u2000-\u200B]", " ", text)

    patterns = find_section_patterns()
    for pats in patterns.values():
        for p in pats:
            if re.search(p, hay, re.IGNORECASE | re.MULTILINE):
                return True

    for line in hay.splitlines():
        if is_probably_legal_section_line(line):
            return True

    return False


# â”€â”€ ë©€í‹°ë¼ì¸ Fallback (ë¼ì¸ ê²½ê³„/ì œê±° ì´ìŠˆ ëŒ€ë¹„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fallback_find_head(full_text: str, rx: re.Pattern) -> int:
    txt = re.sub(r"[\u00A0\u2000-\u200B]", " ", full_text)
    m = rx.search(txt)
    if not m:
        return -1
    return txt[:m.start()].count("\n")


FALLBACK_HEAD_RXS = {
    "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´": re.compile(
        rf"{sec(1)}(?:í™”í•™{sep}ì œí’ˆ{sep}ê³¼{sep}íšŒì‚¬(?:{sep}ì—{sep}ê´€í•œ{sep}ì •ë³´)?|í™”í•™{sep}ì œí’ˆ|ì œí’ˆ{sep}ëª…|í™”í•™{sep}íšŒì‚¬)",
        re.IGNORECASE | re.MULTILINE
    ),
    "ìœ í•´ì„±ìœ„í—˜ì„±": re.compile(
        rf"{sec(2)}(?:(?:ìœ í•´{sep}ì„±{sep}[Â·ãƒ»\.]?{sep}ìœ„í—˜{sep}ì„±)|(?:ìœ í•´{sep}ìœ„í—˜{sep}ì„±)|(?:ìœ í•´{sep}ì„±)|(?:ìœ í•´{sep}ìœ„í—˜)|"
        rf"(?:ìœ„í—˜{sep}ì„±{sep}[Â·ãƒ»\.]?{sep}ìœ í•´{sep}ì„±)|(?:ìœ„í—˜{sep}ìœ í•´{sep}ì„±)|(?:ìœ„í—˜{sep}ìœ í•´)|"
        rf"(?:(?:ìœ í•´|ìœ„í—˜){sep}ì„±{sep}ë°{sep}(?:ìœ í•´|ìœ„í—˜){sep}ì„±)|(?:(?:ìœ í•´|ìœ„í—˜){sep}ë°{sep}(?:ìœ í•´|ìœ„í—˜){sep}ì„±))",
        re.IGNORECASE | re.MULTILINE
    ),
    "êµ¬ì„±ì„±ë¶„": re.compile(
        rf"{sec(3)}(?:êµ¬ì„±{sep}ì„±ë¶„(?:{sep}ì˜{sep}ëª…ì¹­{sep}ë°{sep}(?:í•¨ìœ ?{sep}?ëŸ‰|í•¨ëŸ‰|ì¡°ì„±))?"
        rf"|(?:êµ¬ì„±{sep})?ì„±ë¶„{sep}(?:í‘œ|ì •ë³´)?|ì„±ë¶„{sep}(?:ëª…|ëª…ì¹­){sep}ë°{sep}(?:í•¨ìœ ?{sep}?ëŸ‰|í•¨ëŸ‰)|ì¡°ì„±.*)",
        re.IGNORECASE | re.MULTILINE
    ),
    "ë¬¼ë¦¬í™”í•™ì íŠ¹ì„±": re.compile(
        rf"{sec(9)}(?:ë¬¼ë¦¬{sep}í™”í•™{sep}?ì {sep}(?:íŠ¹ì„±|íŠ¹ì§•)|ë¬¼ë¦¬{sep}í™”í•™{sep}(?:íŠ¹ì„±|íŠ¹ì§•)|ë¬¼ë¦¬{sep}ì {sep}(?:íŠ¹ì„±|íŠ¹ì§•))",
        re.IGNORECASE | re.MULTILINE
    ),
    "ë²•ì ê·œì œ": re.compile(
        rf"{sec(15)}(?:ë²•ì |ë²•ê·œ){sep}ê·œ[ì œì¡”](?:{sep}í˜„í™©)?",
        re.IGNORECASE | re.MULTILINE
    ),
}


# â”€â”€ OCR & í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ocr_page_image(image: Image.Image) -> str:
    config = "--psm 3"
    text = pytesseract.image_to_string(image, lang=TESS_LANG, config=config)
    return text or ""


def extract_text_pages_hybrid(pdf_path: str) -> list[str]:
    texts = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            t = page.extract_text() or ""
            t = strip_page_edges(t)
            texts.append(t)

    need_ocr_idx = [i for i, t in enumerate(texts) if len((t or "").strip()) < OCR_TEXT_MIN_CHARS]
    if ENABLE_OCR and need_ocr_idx:
        try:
            kwargs = {"dpi": OCR_DPI}
            if POPPLER_PATH:
                kwargs["poppler_path"] = POPPLER_PATH

            images = convert_from_path(pdf_path, **kwargs)
            for i in need_ocr_idx:
                try:
                    ocr_t = ocr_page_image(images[i])
                    texts[i] = strip_page_edges(ocr_t)
                except Exception as e:
                    print(f"âš ï¸  OCR ì‹¤íŒ¨ (p{i+1}): {e}")
        except PDFInfoNotInstalledError:
            print("â“˜ Poppler ë¯¸ì„¤ì¹˜ë¡œ OCRì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤. (í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ)")
        except FileNotFoundError as e:
            print(f"â“˜ Poppler ì‹¤í–‰íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\n   â†’ POPPLER_PATHë¥¼ ì˜¬ë°”ë¥¸ bin í´ë”ë¡œ ì§€ì •í•˜ì„¸ìš”.")
        except Exception as e:
            print(f"â“˜ OCR ì´ˆê¸°í™” ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ë¡œ OCRì„ ê±´ë„ˆëœë‹ˆë‹¤: {e}")
    filtered = []
    for t in texts:
        if page_contains_section_head(t):
            filtered.append(t)
            continue
        if is_toc_page(t):
            continue
        filtered.append(t)
    return filtered


# â”€â”€ ë” ì•ˆì „í•œ ëª©ì°¨ ë¸”ë¡ ì œê±°(ì„¹ì…˜ í—¤ë” í¬í•¨ ì‹œ ë¯¸ì œê±°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def would_match_any_section_head(line: str) -> bool:
    patterns = find_section_patterns()
    line_cmp = re.sub(r"[\u00A0\u2000-\u200B]", " ", line)
    for pats in patterns.values():
        for p in pats:
            if re.search(p, line_cmp, re.IGNORECASE):
                return True
    return False


def strip_toc_block(lines: list[str]) -> list[str]:
    """
    ë³¸ë¬¸ ì† ëª©ì°¨ ë¸”ë¡(ì—°ì† ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸)ì„ ë³´ìˆ˜ì ìœ¼ë¡œ ì œê±°í•˜ë˜,
    ë²„í¼ ì•ˆì— 'ì„¹ì…˜ í—¤ë”'ê°€ í•œ ì¤„ì´ë¼ë„ ìˆìœ¼ë©´ ì ˆëŒ€ ì œê±°í•˜ì§€ ì•ŠìŒ.
    """
    out, i, N = [], 0, len(lines)
    while i < N:
        m = re.match(r"^\s*(?:\[(\d{1,2})\]|(\d{1,2})\s*[\.\):])", lines[i])
        if not m:
            out.append(lines[i])
            i += 1
            continue

        j, uniq, buf = i, set(), []
        while j < N:
            mm = re.match(r"^\s*(?:\[(\d{1,2})\]|(\d{1,2})\s*[\.\):])", lines[j])
            if not mm:
                break
            num = int((mm.group(1) or mm.group(2)))
            uniq.add(num)
            buf.append(lines[j])
            j += 1

        if any(would_match_any_section_head(b) for b in buf):
            out.extend(buf)
            i = j
            continue

        seq_count = len(buf)
        avg_len = (sum(len(b) for b in buf) / seq_count) if seq_count else 0
        kw_hits = sum(any(kw in b for kw in TOC_SECTION_KEYS) for b in buf)
        is_toc = (seq_count >= 5 and len(uniq) >= 5 and max(uniq) <= 16
                  and avg_len <= 40 and (kw_hits / seq_count) >= 0.5)

        if is_toc:
            i = j
        else:
            out.extend(buf)
            i = j
    return out


def summarize_sections_for_file(pdf_path: Path):
    print("\n" + "=" * 100)
    print(f"ğŸ“„ íŒŒì¼: {pdf_path.name}")
    print("=" * 100)

    try:
        sections = extract_sections(str(pdf_path))
    except Exception as e:
        print(f"âŒ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return

    found = []
    missing = []

    for key in ALL_SECTION_KEYS:
        title = SECTION_TITLES.get(key, key)
        if key in sections and sections[key].strip():
            found.append(title)
        else:
            missing.append(title)

    print(f"âœ“ ì¶”ì¶œëœ ì„¹ì…˜ ({len(found)}ê°œ):")
    if found:
        for t in found:
            print(f"  - {t}")
    else:
        print("  - ì—†ìŒ")

    print(f"\nâœ— ëˆ„ë½ëœ ì„¹ì…˜ ({len(missing)}ê°œ):")
    if missing:
        for t in missing:
            print(f"  - {t}")
    else:
        print("  - ì—†ìŒ")


# â”€â”€ ì„¹ì…˜ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_sections(pdf_path: str) -> dict:
    page_texts = extract_text_pages_hybrid(pdf_path)

    full_text_raw = "\n".join(page_texts)
    lines = full_text_raw.split("\n")

    lines = remove_repeated_headers(lines)
    lines = strip_toc_block(lines)
    full_text_clean = "\n".join(lines)

    section_patterns = find_section_patterns()
    section_positions = {}
    for section_name, pats in section_patterns.items():
        pos = find_section_start(lines, pats, section_key=section_name)
        if pos != -1:
            section_positions[section_name] = pos

    for key, rx in FALLBACK_HEAD_RXS.items():
        if key not in section_positions:
            idx = fallback_find_head(full_text_raw, rx)
            if idx != -1:
                section_positions[key] = idx

    for key, rx in FALLBACK_HEAD_RXS.items():
        if key not in section_positions:
            idx = fallback_find_head(full_text_clean, rx)
            if idx != -1:
                section_positions[key] = idx

    if not section_positions:
        return {}

    sections = {}
    for section_name, start_pos in sorted(section_positions.items(), key=lambda x: x[1]):
        candidates_after = [p for p in section_positions.values() if p > start_pos]
        default_end = min(candidates_after) if candidates_after else len(lines)
        if section_name in BOUNDARY_NEXT_NUMBER:
            forced_end = find_next_boundary_for(lines, start_pos, BOUNDARY_NEXT_NUMBER[section_name])
            end_pos = min(default_end, forced_end)
        else:
            end_pos = default_end

        body_start = start_pos + 1
        if section_name == "í™”í•™ì œí’ˆê³¼_íšŒì‚¬ì •ë³´":
            start_line = lines[start_pos] if 0 <= start_pos < len(lines) else ""
            if is_product_name_line(start_line):
                body_start = start_pos

        body = []
        for line in lines[body_start:end_pos]:
            if line.strip() and not is_header_line(line):
                body.append(line)
        sections[section_name] = "\n".join(body)

    return sections


def batch_process_msds(dir_path: str):
    base = Path(dir_path)
    if not base.exists():
        print(f"âŒ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dir_path}")
        return

    pdf_files = sorted(base.glob("*.pdf"))
    if not pdf_files:
        print(f"âš ï¸  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {dir_path}")
        return

    print("=" * 80)
    print("MSDS PDF ì„¹ì…˜ ì¶”ì¶œ ë°°ì¹˜ ì‹¤í–‰ (ë””ë ‰í† ë¦¬ ë‹¨ìœ„)")
    print("=" * 80)
    print(f"ëŒ€ìƒ ë””ë ‰í† ë¦¬: {base.resolve()}")
    print(f"PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}\n")

    for pdf in pdf_files:
        summarize_sections_for_file(pdf)

    print("\n" + "=" * 80)
    print("âœ… ë°°ì¹˜ ì¶”ì¶œ ì™„ë£Œ")
    print("=" * 80)


def main_single():
    pdf_path = r"D:\PROJECT\AI\msds-batch-extractor-v0.2\msds\msds\test4.pdf"

    print("=" * 80)
    print("MSDS PDF ì„¹ì…˜ ì¶”ì¶œ (ë‹¨ì¼ íŒŒì¼)")
    print("=" * 80)
    print(f"\níŒŒì¼ ê²½ë¡œ: {pdf_path}\n")

    if not Path(pdf_path).exists():
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return

    try:
        sections = extract_sections(pdf_path)
        if not sections:
            print("âš ï¸  ê²½ê³ : ì¶”ì¶œëœ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        for key, title in SECTION_TITLES.items():
            if key in sections:
                print("\n" + "=" * 80)
                print(f"ğŸ“‹ {title}")
                print("=" * 80)
                content = sections[key]
                if len(content) > 1200:
                    print(content[:1200])
                    print(f"\n... (ì´ {len(content)}ì, ì¼ë¶€ë§Œ í‘œì‹œ)")
                else:
                    print(content)
            else:
                print(f"\nâš ï¸  {title}: ì°¾ì„ ìˆ˜ ì—†ìŒ")

        print("\n" + "=" * 80)
        print("âœ… ë‹¨ì¼ íŒŒì¼ ì¶”ì¶œ ì™„ë£Œ")
        print("=" * 80)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()


# â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_ui():
    st.set_page_config("MSDS Section Extractor (1~16)", layout="wide")
    st.title("MSDS ì„¹ì…˜ ë·°ì–´ (1~16)")

    base_dir = Path(r"C:\Users\ì—„íƒœê· \Desktop\RD\msds-batch-extractor-v0.2\msds\msds")
    pdf_files = sorted(base_dir.glob("*.pdf"))

    if not pdf_files:
        st.error("ì§€ì •í•œ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    selected = st.selectbox(
        "MSDS íŒŒì¼ ì„ íƒ",
        pdf_files,
        format_func=lambda p: p.name,
    )

    if not selected:
        st.info("ì™¼ìª½ì—ì„œ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")
        return

    @st.cache_data(show_spinner=False)
    def _extract_sections_cached(path_str: str):
        return extract_sections(path_str)

    with st.spinner("ì„¹ì…˜ ì¶”ì¶œ ì¤‘..."):
        sections = _extract_sections_cached(str(selected))

    st.markdown(f"### ì„ íƒëœ íŒŒì¼: `{selected.name}`")

    col1, col2 = st.columns(2)

    found_titles = []
    missing_titles = []
    for key in ALL_SECTION_KEYS:
        title = SECTION_TITLES.get(key, key)
        content = sections.get(key, "").strip() if sections else ""
        if content:
            found_titles.append(title)
        else:
            missing_titles.append(title)

    with col1:
        st.subheader("âœ“ ì¶”ì¶œëœ ì„¹ì…˜")
        st.write(f"{len(found_titles)}/{len(ALL_SECTION_KEYS)}")
        for t in found_titles:
            st.write("âœ… " + t)

    with col2:
        st.subheader("âœ— ëˆ„ë½ëœ ì„¹ì…˜")
        if missing_titles:
            for t in missing_titles:
                st.write("âš ï¸ " + t)
        else:
            st.write("ì—†ìŒ")

    st.markdown("---")

    for key in ALL_SECTION_KEYS:
        title = SECTION_TITLES.get(key, key)
        content = sections.get(key, "").strip() if sections else ""

        # expanded=bool(content) ë¡œ ì´ë¯¸ ë¬¸ìì—´ â†’ bool ë³€í™˜ ì™„ë£Œ
        with st.expander(title, expanded=bool(content)):
            if not content:
                st.info("ì´ ì„¹ì…˜ì€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            else:
                st.text(content)


if __name__ == "__main__":
    run_ui()
    # main_single()
    # batch_process_msds(r"D:\PROJECT\AI\msds-batch-extractor-v0.2\msds\msds")
